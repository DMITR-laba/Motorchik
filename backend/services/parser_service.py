"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å —Å–∞–π—Ç–∞ aaa-motors.ru
"""
import re
import json
import time
import httpx
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from models.database import ParsedCar, ParsedCarPicture
import logging

logger = logging.getLogger(__name__)


class AAAMotorsParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ aaa-motors.ru"""
    
    def __init__(self, db_session: Session, base_url: str = "https://aaa-motors.ru"):
        self.db = db_session
        self.base_url = base_url
        self.session = None
        self.stats = {
            "total_parsed": 0,
            "total_errors": 0,
            "current_page": 0
        }
        self.is_running = False
        
    def _create_session(self):
        """–°–æ–∑–¥–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        if self.session is None:
            self.session = httpx.Client(
                timeout=30.0,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1"
                },
                follow_redirects=True
            )
        return self.session
    
    def _extract_number(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
        numbers = re.findall(r'\d+', str(text).replace(' ', ''))
        if numbers:
            return int(numbers[0])
        return None
    
    def _extract_price(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ü–∏—Ñ—Ä—ã –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        price_clean = re.sub(r'[^\d\s,.]', '', str(text))
        return price_clean.strip() if price_clean.strip() else None
    
    def _parse_car_page(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è"""
        try:
            session = self._create_session()
            response = session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            car_data = {
                "source_url": url,
                "mark": None,
                "model": None,
                "city": None,
                "price": None,
                "manufacture_year": None,
                "body_type": None,
                "fuel_type": None,
                "gear_box_type": None,
                "driving_gear_type": None,
                "engine_vol": None,
                "power": None,
                "color": None,
                "mileage": None,
                "characteristics": {},
                "pictures": []
            }
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–º–∞—Ä–∫–∞ –∏ –º–æ–¥–µ–ª—å)
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                # –§–æ—Ä–º–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å: "BMW X5 2024" –∏–ª–∏ "BMW X5 –≤ –ú–æ—Å–∫–≤–µ"
                title_parts = title_text.split()
                if len(title_parts) >= 2:
                    car_data["mark"] = title_parts[0]
                    car_data["model"] = " ".join(title_parts[1:3]) if len(title_parts) >= 3 else title_parts[1]
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã
            price_selectors = [
                soup.find(class_=re.compile(r'price', re.I)),
                soup.find(id=re.compile(r'price', re.I)),
                soup.find(string=re.compile(r'—Ü–µ–Ω–∞', re.I)),
                soup.find('span', string=re.compile(r'\d+.*‚ÇΩ', re.I)),
            ]
            for price_elem in price_selectors:
                if price_elem:
                    price_text = price_elem.get_text() if hasattr(price_elem, 'get_text') else str(price_elem)
                    car_data["price"] = self._extract_price(price_text)
                    if car_data["price"]:
                        break
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ —Å–ø–∏—Å–∫–∞
            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
            specs_tables = soup.find_all(['table', 'dl', 'div'], class_=re.compile(r'spec|characteristic|param', re.I))
            for table in specs_tables:
                rows = table.find_all(['tr', 'dt']) if table.name != 'div' else table.find_all('div')
                for row in rows:
                    cells = row.find_all(['td', 'dd', 'span'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True).lower()
                        value = cells[1].get_text(strip=True)
                        
                        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–π –Ω–∞ –ø–æ–ª—è –ë–î
                        if '–º–∞—Ä–∫–∞' in key or 'brand' in key:
                            car_data["mark"] = value
                        elif '–º–æ–¥–µ–ª—å' in key or 'model' in key:
                            car_data["model"] = value
                        elif '–≥–æ—Ä–æ–¥' in key or 'city' in key or '–ª–æ–∫–∞—Ü–∏—è' in key:
                            car_data["city"] = value
                        elif '–≥–æ–¥' in key or 'year' in key:
                            car_data["manufacture_year"] = self._extract_number(value)
                        elif '–∫—É–∑–æ–≤' in key or 'body' in key:
                            car_data["body_type"] = value
                        elif '—Ç–æ–ø–ª–∏–≤–æ' in key or 'fuel' in key:
                            car_data["fuel_type"] = value
                        elif '–∫–æ—Ä–æ–±–∫–∞' in key or 'transmission' in key or 'gearbox' in key:
                            car_data["gear_box_type"] = value
                        elif '–ø—Ä–∏–≤–æ–¥' in key or 'drive' in key:
                            car_data["driving_gear_type"] = value
                        elif '–æ–±—ä–µ–º' in key or 'engine' in key or '–¥–≤–∏–≥–∞—Ç–µ–ª—å' in key:
                            vol = self._extract_number(value)
                            if vol:
                                car_data["engine_vol"] = vol
                        elif '–º–æ—â–Ω–æ—Å—Ç—å' in key or 'power' in key:
                            car_data["power"] = value
                        elif '—Ü–≤–µ—Ç' in key or 'color' in key:
                            car_data["color"] = value
                        elif '–ø—Ä–æ–±–µ–≥' in key or 'mileage' in key:
                            car_data["mileage"] = self._extract_number(value)
                        else:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ characteristics
                            car_data["characteristics"][key] = value
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
            img_selectors = [
                soup.find_all('img', class_=re.compile(r'car|photo|image|gallery', re.I)),
                soup.find_all('img', src=re.compile(r'car|auto|photo', re.I)),
            ]
            
            all_images = set()
            for selector_list in img_selectors:
                for img in selector_list:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    if src:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = urljoin(self.base_url, src)
                        elif not src.startswith('http'):
                            src = urljoin(self.base_url, src)
                        all_images.add(src)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–æ—Ç–æ
            sorted_images = sorted(list(all_images))
            for idx, img_url in enumerate(sorted_images[:20]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 —Ñ–æ—Ç–æ
                car_data["pictures"].append({
                    "image_url": img_url,
                    "seqno": idx
                })
            
            return car_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            self.stats["total_errors"] += 1
            return None
    
    def _find_car_links(self, page_url: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–∞–ª–æ–≥–∞"""
        try:
            session = self._create_session()
            response = session.get(page_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            car_links = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è aaa-motors.ru)
            # –§–æ—Ä–º–∞—Ç: /sale/used/mark/model/id –∏–ª–∏ /sale/new/mark/model/id
            link_selectors = [
                # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è aaa-motors.ru (–∫–ª–∞—Å—Å js-item)
                soup.find_all('a', class_=re.compile(r'js-item|item-row', re.I)),
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ href
                soup.find_all('a', href=re.compile(r'/sale/(used|new)/', re.I)),
                soup.find_all('a', href=re.compile(r'/car/|/auto/|/vehicle/|/offer/', re.I)),
                soup.find_all('a', class_=re.compile(r'car|auto|vehicle|offer|card', re.I)),
            ]
            
            for selector_list in link_selectors:
                for link in selector_list:
                    href = link.get('href')
                    if not href:
                        continue
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = urljoin(self.base_url, href)
                    elif not href.startswith('http'):
                        href = urljoin(self.base_url, href)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å
                    # –§–æ—Ä–º–∞—Ç aaa-motors.ru: /sale/used/daewoo/matiz/cd64d5
                    if any(pattern in href.lower() for pattern in [
                        '/sale/used/', '/sale/new/', '/car/', '/auto/', '/vehicle/', '/offer/'
                    ]):
                        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
                        if '?' in href:
                            href = href.split('?')[0]
                        if '#' in href:
                            href = href.split('#')[0]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å (–º–∏–Ω–∏–º—É–º 2 —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ /sale/used/)
                        parts = href.rstrip('/').split('/')
                        if len(parts) >= 5:  # https://, domain, sale, used, mark, model, id
                            car_links.append(href)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
            seen = set()
            unique_links = []
            for link in car_links:
                if link not in seen:
                    seen.add(link)
                    unique_links.append(link)
            
            return unique_links
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url}: {e}")
            return []
    
    def _find_catalog_pages(self) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        catalog_pages = []
        
        try:
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ URL –¥–ª—è aaa-motors.ru
            catalog_urls = [
                f"{self.base_url}/catalog",  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥
                f"{self.base_url}/sale/used",  # –ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
                f"{self.base_url}/sale/new",  # –ù–æ–≤—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
                f"{self.base_url}/cars",
                f"{self.base_url}/auto",
                f"{self.base_url}/offers",
                f"{self.base_url}/",
            ]
            
            session = self._create_session()
            for catalog_url in catalog_urls:
                try:
                    response = session.get(catalog_url)
                    if response.status_code == 200:
                        catalog_pages.append(catalog_url)
                        
                        # –ü–∞—Ä—Å–∏–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        pagination = soup.find_all('a', href=re.compile(r'page|p=\d+', re.I))
                        for page_link in pagination:
                            href = page_link.get('href')
                            if href:
                                if href.startswith('//'):
                                    href = 'https:' + href
                                elif href.startswith('/'):
                                    href = urljoin(self.base_url, href)
                                elif not href.startswith('http'):
                                    href = urljoin(self.base_url, href)
                                if href not in catalog_pages:
                                    catalog_pages.append(href)
                        break
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {catalog_url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞: {e}")
        
        return catalog_pages
    
    def _save_car(self, car_data: Dict[str, Any]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—å —Å —Ç–∞–∫–∏–º URL
            existing = self.db.query(ParsedCar).filter(
                ParsedCar.source_url == car_data["source_url"]
            ).first()
            
            if existing:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
                for key, value in car_data.items():
                    if key != "pictures" and key != "characteristics" and hasattr(existing, key):
                        setattr(existing, key, value)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                if car_data.get("characteristics"):
                    existing.characteristics = json.dumps(car_data["characteristics"], ensure_ascii=False)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (—É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ)
                self.db.query(ParsedCarPicture).filter(
                    ParsedCarPicture.parsed_car_id == existing.id
                ).delete()
                
                parsed_car = existing
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                parsed_car = ParsedCar(
                    source_url=car_data["source_url"],
                    mark=car_data.get("mark"),
                    model=car_data.get("model"),
                    city=car_data.get("city"),
                    price=car_data.get("price"),
                    manufacture_year=car_data.get("manufacture_year"),
                    body_type=car_data.get("body_type"),
                    fuel_type=car_data.get("fuel_type"),
                    gear_box_type=car_data.get("gear_box_type"),
                    driving_gear_type=car_data.get("driving_gear_type"),
                    engine_vol=car_data.get("engine_vol"),
                    power=car_data.get("power"),
                    color=car_data.get("color"),
                    mileage=car_data.get("mileage"),
                    characteristics=json.dumps(car_data.get("characteristics", {}), ensure_ascii=False) if car_data.get("characteristics") else None,
                    is_active=True
                )
                self.db.add(parsed_car)
                self.db.flush()  # –ü–æ–ª—É—á–∞–µ–º ID
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            for pic_data in car_data.get("pictures", []):
                picture = ParsedCarPicture(
                    parsed_car_id=parsed_car.id,
                    image_url=pic_data["image_url"],
                    seqno=pic_data.get("seqno", 0)
                )
                self.db.add(picture)
            
            self.db.commit()
            self.stats["total_parsed"] += 1
            return True
            
        except IntegrityError as e:
            self.db.rollback()
            logger.warning(f"–î—É–±–ª–∏–∫–∞—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è {car_data.get('source_url')}: {e}")
            return False
        except Exception as e:
            self.db.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {e}")
            self.stats["total_errors"] += 1
            return False
    
    def clear_all_data(self) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü –ø–∞—Ä—Å–∏–Ω–≥–∞ (–≤–∫–ª—é—á–∞—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
            cars_count = self.db.query(ParsedCar).count()
            pictures_count = self.db.query(ParsedCarPicture).count()
            
            logger.info(f"üóëÔ∏è –ù–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {cars_count} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {pictures_count} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
            
            if cars_count == 0 and pictures_count == 0:
                logger.info("‚úÖ –î–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—Ç, –±–∞–∑–∞ —É–∂–µ –ø—É—Å—Ç–∞")
                return 0
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–¥–æ–ª–∂–Ω—ã —É–¥–∞–ª—è—Ç—å—Å—è –ø–µ—Ä–≤—ã–º–∏ –∏–∑-–∑–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π)
            deleted_pictures = self.db.query(ParsedCarPicture).delete(synchronize_session=False)
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {deleted_pictures}")
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ (–í–°–ï, –≤–∫–ª—é—á–∞—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ)
            deleted_cars = self.db.query(ParsedCar).delete(synchronize_session=False)
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {deleted_cars}")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
            self.db.commit()
            logger.info(f"‚úÖ –ö–æ–º–º–∏—Ç –æ—á–∏—Å—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–µ–Ω—ã
            remaining_cars = self.db.query(ParsedCar).count()
            remaining_pictures = self.db.query(ParsedCarPicture).count()
            
            if remaining_cars > 0 or remaining_pictures > 0:
                logger.warning(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–∏—Ç—å –µ—â–µ —Ä–∞–∑
                try:
                    logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π...")
                    self.db.query(ParsedCarPicture).delete(synchronize_session=False)
                    self.db.query(ParsedCar).delete(synchronize_session=False)
                    self.db.commit()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑
                    remaining_cars = self.db.query(ParsedCar).count()
                    remaining_pictures = self.db.query(ParsedCarPicture).count()
                    if remaining_cars == 0 and remaining_pictures == 0:
                        logger.info(f"‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                    else:
                        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è: {e}", exc_info=True)
            
            logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {deleted_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {deleted_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
            return deleted_cars
        except Exception as e:
            self.db.rollback()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
            raise
    
    def parse(self, max_pages: Optional[int] = None, max_cars: Optional[int] = None, delay: float = 1.0, clear_before: bool = True) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        
        Args:
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            max_cars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
            clear_before: –û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        """
        self.is_running = True
        self.stats = {
            "total_parsed": 0,
            "total_errors": 0,
            "current_page": 0
        }
        
        try:
            # –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            if clear_before:
                logger.info("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º...")
                deleted_count = self.clear_all_data()
                logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø–∞—Ä—Å–∏–Ω–≥–∞")
            
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
            catalog_pages = self._find_catalog_pages()
            if not catalog_pages:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞")
                return {
                    "status": "error",
                    "message": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞",
                    **self.stats
                }
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            if max_pages:
                catalog_pages = catalog_pages[:max_pages]
            
            all_car_links = []
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
            for page_url in catalog_pages:
                if not self.is_running:
                    break
                    
                self.stats["current_page"] += 1
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {self.stats['current_page']}: {page_url}")
                
                car_links = self._find_car_links(page_url)
                all_car_links.extend(car_links)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if delay > 0:
                    time.sleep(delay)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_car_links = list(set(all_car_links))
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_car_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            if max_cars:
                unique_car_links = unique_car_links[:max_cars]
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å
            for idx, car_url in enumerate(unique_car_links):
                if not self.is_running:
                    break
                
                if max_cars and self.stats["total_parsed"] >= max_cars:
                    break
                
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∞–≤—Ç–æ–º–æ–±–∏–ª—è {idx + 1}/{len(unique_car_links)}: {car_url}")
                
                car_data = self._parse_car_page(car_url)
                if car_data:
                    self._save_car(car_data)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if delay > 0 and idx < len(unique_car_links) - 1:
                    time.sleep(delay)
            
            return {
                "status": "completed",
                "message": f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.stats['total_parsed']} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π",
                **self.stats
            }
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return {
                "status": "error",
                "message": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}",
                **self.stats
            }
        finally:
            self.is_running = False
            if self.session:
                self.session.close()
                self.session = None
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥"""
        self.is_running = False
    
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        return {
            "status": "running" if self.is_running else "stopped",
            **self.stats
        }

