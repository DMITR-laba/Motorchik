"""
–ì–ª–∞–≤–Ω—ã–π –ò–ò-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
–¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ —Å–∏—Å—Ç–µ–º–µ –∞–≤—Ç–æ—Å–∞–ª–æ–Ω–∞
"""
from typing import Optional, Dict, Any, List
from enum import Enum
import json
import os
from datetime import datetime
from services.langchain_llm_service import LangChainLLMService
from services.ai_service import AIService
from services.ollama_utils import find_working_ollama_url
from app.core.config import settings


class TaskType(Enum):
    """–¢–∏–ø—ã –∑–∞–¥–∞—á –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏"""
    QUERY_ANALYSIS = "query_analysis"
    FUZZY_INTERPRETATION = "fuzzy_interpretation"
    FILTER_RELAXATION = "filter_relaxation"
    RECOMMENDATION = "recommendation"
    SQL_GENERATION = "sql_generation"
    RESPONSE_GENERATION = "response_generation"
    SEARCH_INTENT_ANALYSIS = "search_intent_analysis"
    QUERY_REFINEMENT = "query_refinement"
    RESULT_PROCESSING = "result_processing"
    RELATION_ANALYSIS = "relation_analysis"
    EMOTION_ANALYSIS = "emotion_analysis"
    QUESTION_GENERATION = "question_generation"
    ANSWER_GENERATION = "answer_generation"
    PROACTIVE_SUGGESTIONS = "proactive_suggestions"


class Complexity(Enum):
    """–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏"""
    LIGHT = "light"
    MEDIUM = "medium"
    HEAVY = "heavy"


class AIModelOrchestratorService:
    """–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π LLM"""
    
    def __init__(self, config_path: str = "backend/ai_model_config.json"):
        self.config_path = config_path
        self.langchain_service = LangChainLLMService()
        self.ai_service = AIService()
        self.config = self._load_config()
        self._model_cache: Dict[str, Any] = {}
        self._performance_metrics: Dict[str, Dict[str, Any]] = {}
        self._available_models: List[str] = []
    
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π"""
        default_config = {
            "task_model_mapping": {
                "query_analysis": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "fuzzy_interpretation": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "filter_relaxation": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "recommendation": {
                    "primary": "ollama:llama3:70b",
                    "fallback": "ollama:mixtral:8b",
                    "complexity": "heavy"
                },
                "sql_generation": {
                    "primary": "ollama:codellama:34b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "response_generation": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "search_intent_analysis": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "query_refinement": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "result_processing": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "relation_analysis": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "emotion_analysis": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "question_generation": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                },
                "answer_generation": {
                    "primary": "ollama:llama3:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "light"
                },
                "proactive_suggestions": {
                    "primary": "ollama:mixtral:8b",
                    "fallback": "ollama:llama3:8b",
                    "complexity": "medium"
                }
            },
            "user_overrides": {
                "enabled": True,
                "respect_user_settings": True
            },
            "performance_tracking": {
                "enabled": True,
                "cache_size": 100,
                "track_success_rate": True,
                "track_response_time": True
            },
            "model_availability_check": {
                "enabled": True,
                "check_interval_seconds": 300,
                "auto_fallback": True
            }
        }
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å (–±–µ–∑ backend/ –µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ–º –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
            config_file = self.config_path
            if config_file.startswith("backend/") and not os.path.exists("backend"):
                config_file = config_file.replace("backend/", "")
            
            if os.path.exists(config_file):
                with open(config_file, "r", encoding="utf-8") as f:
                    user_config = json.load(f)
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
                    default_config.update(user_config)
                    # –û–±–Ω–æ–≤–ª—è–µ–º task_model_mapping –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                    if "task_model_mapping" in user_config:
                        default_config["task_model_mapping"].update(user_config["task_model_mapping"])
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        return default_config
    
    def _load_user_settings(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ ai_settings.json –∏ sql_agent_settings.json"""
        user_overrides = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ ai_settings.json
        try:
            if os.path.exists("backend/ai_settings.json"):
                with open("backend/ai_settings.json", "r", encoding="utf-8") as f:
                    ai_settings = json.load(f)
                    response_model = ai_settings.get("response_model", "")
                    if response_model:
                        user_overrides["response_generation"] = response_model
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ai_settings.json: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ sql_agent_settings.json
        try:
            if os.path.exists("backend/sql_agent_settings.json"):
                with open("backend/sql_agent_settings.json", "r", encoding="utf-8") as f:
                    sql_settings = json.load(f)
                    sql_model = sql_settings.get("sql_model", "")
                    if sql_model:
                        user_overrides["sql_generation"] = sql_model
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sql_agent_settings.json: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ ai_model_config.json (–µ—Å–ª–∏ –µ—Å—Ç—å user_model_overrides)
        try:
            if "user_model_overrides" in self.config:
                user_overrides.update(self.config["user_model_overrides"])
        except Exception:
            pass
        
        return user_overrides
    
    def _save_user_settings(self, task_type: str, model: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –≤ ai_model_config.json"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ñ–∏–≥
            config = self._load_config()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º user_model_overrides –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if "user_model_overrides" not in config:
                config["user_model_overrides"] = {}
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏
            config["user_model_overrides"][task_type] = model
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            config_dir = os.path.dirname(self.config_path)
            if config_dir:
                os.makedirs(config_dir, exist_ok=True)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å (–±–µ–∑ backend/ –µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ–º –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
            config_file = self.config_path
            if config_file.startswith("backend/") and not os.path.exists("backend"):
                config_file = config_file.replace("backend/", "")
            
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ñ–∏–≥
            self.config = config
            
            # –û–±–Ω–æ–≤–ª—è–µ–º user_model_overrides –≤ –∫–æ–Ω—Ñ–∏–≥–µ –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            if "user_model_overrides" not in self.config:
                self.config["user_model_overrides"] = {}
            self.config["user_model_overrides"][task_type] = model
            
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å {model} –¥–ª—è –∑–∞–¥–∞—á–∏ {task_type}")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def save_multiple_models(self, models: Dict[str, str]) -> Dict[str, bool]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á"""
        results = {}
        for task_type, model in models.items():
            results[task_type] = self._save_user_settings(task_type, model)
        return results
    
    async def select_model_for_task(
        self,
        task_type: TaskType,
        task_complexity: Optional[Complexity] = None,
        user_override: Optional[str] = None
    ) -> str:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:
        1. user_override (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)
        2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ ai_settings.json (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫)
        3. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        4. –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á)
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            task_complexity: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            user_override: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Returns:
            str: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "provider:model_name"
        """
        task_key = task_type.value
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if user_override and user_override.strip():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            await self._ensure_model_available(user_override)
            return user_override
        
        # 2. –ü–†–ò–û–†–ò–¢–ï–¢: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ ai_settings.json (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫)
        try:
            import os
            import json
            ai_settings_paths = [
                "backend/ai_settings.json",
                "ai_settings.json",
                os.path.join(os.path.dirname(os.path.dirname(__file__)), "ai_settings.json")
            ]
            
            for settings_path in ai_settings_paths:
                if os.path.exists(settings_path):
                    with open(settings_path, "r", encoding="utf-8") as f:
                        ai_settings = json.load(f)
                        response_model = ai_settings.get("response_model", "")
                        
                        # –î–ª—è response_generation –∏—Å–ø–æ–ª—å–∑—É–µ–º response_model –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                        if task_key == "response_generation" and response_model and response_model.strip():
                            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –∏–∑ ai_settings.json –¥–ª—è {task_key}: {response_model}")
                            await self._ensure_model_available(response_model)
                            return response_model
                        
                        # –î–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º response_model –∫–∞–∫ –æ–±—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
                        break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ai_settings.json: {e}")
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        user_overrides = self._load_user_settings()
        if self.config.get("user_overrides", {}).get("enabled", True):
            if task_key in user_overrides:
                user_model = user_overrides[task_key]
                if user_model and user_model.strip():
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {task_key}: {user_model}")
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    await self._ensure_model_available(user_model)
                    return user_model
        
        # 4. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á) - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        selected_model = await self._ai_select_model(task_type, task_complexity)
        
        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        await self._ensure_model_available(selected_model)
        
        return selected_model
    
    async def _ai_select_model(
        self,
        task_type: TaskType,
        task_complexity: Optional[Complexity] = None
    ) -> str:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ò–ò –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
            available_models = await self.get_available_models()
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–¥–∞—á–∏
            task_key = task_type.value
            task_config = self.config.get("task_model_mapping", {}).get(task_key, {})
            if not task_config:
                task_config = self.config.get("task_model_mapping", {}).get("response_generation", {})
            
            primary_model = task_config.get("primary", "ollama:llama3:8b")
            fallback_model = task_config.get("fallback", "ollama:llama3:8b")
            complexity = task_complexity.value if task_complexity else task_config.get("complexity", "light")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω –ª–∏ –ò–ò –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π
            ai_selection_config = self.config.get("ai_model_selection", {})
            if not ai_selection_config.get("enabled", True) or not ai_selection_config.get("use_ai_for_selection", True):
                return primary_model
            
            min_models = ai_selection_config.get("min_models_for_ai", 3)
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞–ª–æ –∏–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if len(available_models) < min_models or complexity == "light":
                return primary_model
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Ä–µ–∫—É—Ä—Å–∏–∏)
            try:
                llm = self.langchain_service.get_llm("ollama:llama3:8b", None)
            except:
                # –ï—Å–ª–∏ –¥–∞–∂–µ –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                return primary_model
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ò–ò
            prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤—ã–±–æ—Ä—É LLM –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á.

–ó–ê–î–ê–ß–ê: {task_key}
–°–õ–û–ñ–ù–û–°–¢–¨: {complexity}
–î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò: {', '.join(available_models[:10])}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –ú–û–î–ï–õ–¨: {primary_model}
FALLBACK –ú–û–î–ï–õ–¨: {fallback_model}

–í—ã–±–µ—Ä–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏, —É—á–∏—Ç—ã–≤–∞—è:
1. –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏ ({complexity})
2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
3. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å
4. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É –æ—Ç–≤–µ—Ç–∞

–ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –ª–µ–≥–∫–∞—è - –≤—ã–±–µ—Ä–∏ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å (llama3:8b, mixtral:8b)
–ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Å—Ä–µ–¥–Ω—è—è - –≤—ã–±–µ—Ä–∏ —Å—Ä–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å (mixtral:8b, codellama:34b)
–ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç—è–∂–µ–ª–∞—è - –≤—ã–±–µ—Ä–∏ —Ç—è–∂–µ–ª—É—é –º–æ–¥–µ–ª—å (llama3:70b, mistral-large)

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "ollama:model_name" –∏–ª–∏ "mistral:model_name" –∏ —Ç.–¥.
–ï—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–π –µ—ë, –∏–Ω–∞—á–µ –≤—ã–±–µ—Ä–∏ –ª—É—á—à—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö."""
            
            from langchain_core.output_parsers import StrOutputParser
            from langchain_core.prompts import ChatPromptTemplate
            
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤—ã–±–æ—Ä—É LLM –º–æ–¥–µ–ª–µ–π. –í—ã–±–∏—Ä–∞–µ—à—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏."),
                ("human", prompt)
            ])
            
            chain = prompt_template | llm | StrOutputParser()
            response = await chain.ainvoke({})
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            ai_selected = response.strip()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            if ai_selected and (ai_selected in available_models or ai_selected == primary_model):
                print(f"ü§ñ –ò–ò –≤—ã–±—Ä–∞–ª –º–æ–¥–µ–ª—å: {ai_selected}")
                return ai_selected
            else:
                # –ï—Å–ª–∏ –ò–ò –≤—ã–±—Ä–∞–ª –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é
                print(f"‚ö†Ô∏è –ò–ò –≤—ã–±—Ä–∞–ª –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å {ai_selected}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é: {primary_model}")
                return primary_model
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ò–ò –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
            # Fallback –Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            task_key = task_type.value
            task_config = self.config.get("task_model_mapping", {}).get(task_key, {})
            if not task_config:
                task_config = self.config.get("task_model_mapping", {}).get("response_generation", {})
            return task_config.get("primary", "ollama:llama3:8b")
    
    async def _ensure_model_available(self, model_config: str):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ—ë –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        –í–ê–ñ–ù–û: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ API.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω–∞ –ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á–µ–Ω–∞ (False), —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ –±–µ–∑ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        auto_load_config = self.config.get("auto_model_loading", {})
        if not auto_load_config.get("enabled", False) or not auto_load_config.get("auto_load_missing_models", False):
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            if model_config.startswith("ollama:"):
                model_name = model_config.replace("ollama:", "")
                available_models = await self.get_available_models()
                if model_config not in available_models:
                    print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ API /api/models/load –¥–ª—è —Ä—É—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.")
            return
        
        if not model_config.startswith("ollama:"):
            # –î–ª—è –Ω–µ-Ollama –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            return
        
        model_name = model_config.replace("ollama:", "")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        working_url = await find_working_ollama_url(timeout=2.0)
        if not working_url:
            print(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
        available_models = await self.get_available_models()
        model_full_name = f"ollama:{model_name}"
        
        if model_full_name in available_models:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –¥–æ—Å—Ç—É–ø–Ω–∞")
            return
        
        # –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å
        print(f"üì• –ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É...")
        try:
            timeout = auto_load_config.get("load_timeout_seconds", 300)
            await self._auto_load_model(model_name, working_url, timeout)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Å fallback –º–æ–¥–µ–ª—å—é
    
    async def _auto_load_model(self, model_name: str, ollama_url: str, timeout: float = 300.0):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Ollama API —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "llama3:8b")
            ollama_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        import httpx
        
        try:
            print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ {model_name}...")
            
            async with httpx.AsyncClient(timeout=timeout) as client:
                # –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
                async with client.stream(
                    "POST",
                    f"{ollama_url}/api/pull",
                    json={"name": model_name},
                    timeout=timeout
                ) as response:
                    if response.status_code != 200:
                        raise Exception(f"HTTP {response.status_code}: {await response.aread()}")
                    
                    # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏
                    last_percent = -1
                    async for line in response.aiter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                status = data.get("status", "")
                                
                                if status == "success":
                                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
                                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                                    self._available_models = await self.get_available_models()
                                    break
                                elif status == "downloading" or status == "pulling":
                                    completed = data.get("completed", 0)
                                    total = data.get("total", 0)
                                    if total > 0:
                                        percent = int((completed / total) * 100)
                                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞ 5% –∏–ª–∏ –±–æ–ª—å—à–µ
                                        if percent - last_percent >= 5 or percent == 100:
                                            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ {model_name}: {percent}% ({self._format_size(completed)} / {self._format_size(total)})")
                                            last_percent = percent
                                elif status == "error":
                                    error_msg = data.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
                                    raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {error_msg}")
                                    
                            except json.JSONDecodeError:
                                continue
                            except Exception as e:
                                if "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏" in str(e):
                                    raise
                                continue
                    
        except httpx.TimeoutException:
            print(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name} (>{timeout} —Å–µ–∫), –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É")
            raise Exception(f"–¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {str(e)}")
    
    def _format_size(self, size_bytes: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"
    
    async def get_llm_for_task_async(
        self,
        task_type: TaskType,
        task_complexity: Optional[Complexity] = None,
        user_override: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        –ü–æ–ª—É—á–∞–µ—Ç LLM –æ–±—ä–µ–∫—Ç –¥–ª—è –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ LangChainLLMService (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            task_complexity: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            user_override: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            api_key: API –∫–ª—é—á (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
        
        Returns:
            BaseChatModel: LangChain LLM –æ–±—ä–µ–∫—Ç
        """
        model_config = await self.select_model_for_task(task_type, task_complexity, user_override)
        
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
        if not api_key and (model_config.startswith("mistral:") or 
                           model_config.startswith("openai:") or 
                           model_config.startswith("anthropic:")):
            # –ó–∞–≥—Ä—É–∂–∞–µ–º API –∫–ª—é—á –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            try:
                if os.path.exists("backend/ai_settings.json"):
                    with open("backend/ai_settings.json", "r", encoding="utf-8") as f:
                        ai_settings = json.load(f)
                        api_key = ai_settings.get("api_key", "")
            except Exception:
                pass
        
        return self.langchain_service.get_llm(model_config, api_key)
    
    def get_llm_for_task(
        self,
        task_type: TaskType,
        task_complexity: Optional[Complexity] = None,
        user_override: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        –ü–æ–ª—É—á–∞–µ—Ç LLM –æ–±—ä–µ–∫—Ç –¥–ª—è –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ LangChainLLMService (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            task_complexity: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            user_override: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            api_key: API –∫–ª—é—á (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
        
        Returns:
            BaseChatModel: LangChain LLM –æ–±—ä–µ–∫—Ç
        """
        import asyncio
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # –ï—Å–ª–∏ —Ü–∏–∫–ª —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –º–æ–¥–µ–ª—å
                print("‚ö†Ô∏è Event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                model_config = user_override or "ollama:llama3:8b"
            else:
                model_config = loop.run_until_complete(
                    self.select_model_for_task(task_type, task_complexity, user_override)
                )
        except RuntimeError:
            # –ù–µ—Ç event loop, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            model_config = asyncio.run(
                self.select_model_for_task(task_type, task_complexity, user_override)
            )
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            model_config = user_override or "ollama:llama3:8b"
        
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
        if not api_key and (model_config.startswith("mistral:") or 
                           model_config.startswith("openai:") or 
                           model_config.startswith("anthropic:")):
            # –ó–∞–≥—Ä—É–∂–∞–µ–º API –∫–ª—é—á –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            try:
                if os.path.exists("backend/ai_settings.json"):
                    with open("backend/ai_settings.json", "r", encoding="utf-8") as f:
                        ai_settings = json.load(f)
                        api_key = ai_settings.get("api_key", "")
            except Exception:
                pass
        
        return self.langchain_service.get_llm(model_config, api_key)
    
    async def register_model_usage(
        self,
        model_name: str,
        task_type: TaskType,
        success: bool,
        response_time: Optional[float] = None
    ):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫"""
        if not self.config.get("performance_tracking", {}).get("enabled", True):
            return
        
        metric_key = f"{model_name}:{task_type.value}"
        
        if metric_key not in self._performance_metrics:
            self._performance_metrics[metric_key] = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "total_response_time": 0.0,
                "average_response_time": 0.0,
                "success_rate": 0.0
            }
        
        metrics = self._performance_metrics[metric_key]
        metrics["total_requests"] += 1
        
        if success:
            metrics["successful_requests"] += 1
        else:
            metrics["failed_requests"] += 1
        
        if response_time is not None:
            metrics["total_response_time"] += response_time
            metrics["average_response_time"] = metrics["total_response_time"] / metrics["total_requests"]
        
        metrics["success_rate"] = metrics["successful_requests"] / metrics["total_requests"] if metrics["total_requests"] > 0 else 0.0
    
    def get_model_performance(self, model_name: Optional[str] = None, task_type: Optional[TaskType] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        if model_name and task_type:
            metric_key = f"{model_name}:{task_type.value}"
            return self._performance_metrics.get(metric_key, {})
        elif model_name:
            # –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
            return {k: v for k, v in self._performance_metrics.items() if k.startswith(f"{model_name}:")}
        elif task_type:
            # –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∑–∞–¥–∞—á–∏
            return {k: v for k, v in self._performance_metrics.items() if k.endswith(f":{task_type.value}")}
        else:
            # –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏
            return self._performance_metrics
    
    async def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Ollama + API)"""
        available_models = []
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ Ollama
        try:
            ollama_models = await self.ai_service.get_ollama_models()
            available_models.extend([f"ollama:{model.get('name', '')}" for model in ollama_models if model.get('name')])
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π Ollama: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ API –º–æ–¥–µ–ª–∏
        api_models = [
            # Mistral
            "mistral:mistral-small-latest",
            "mistral:mistral-medium-latest",
            "mistral:mistral-large-latest",
            # OpenAI
            "openai:gpt-4",
            "openai:gpt-4-turbo",
            "openai:gpt-3.5-turbo",
            # Anthropic
            "anthropic:claude-3-opus-20240229",
            "anthropic:claude-3-sonnet-20240229",
            "anthropic:claude-3-haiku-20240307",
            # DeepSeek (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
            "deepseek:deepseek-chat",
            "deepseek:deepseek-reasoner"
        ]
        
        available_models.extend(api_models)
        
        return available_models
    
    def reload_config(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        self.config = self._load_config()
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞")

