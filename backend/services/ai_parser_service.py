"""
–ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP, ML –∏ Ollama
"""
import re
import json
import time
import httpx
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from models.database import ParsedCar, ParsedCarPicture
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

# NLP –∏–º–ø–æ—Ä—Ç—ã
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    spacy = None
    SPACY_AVAILABLE = False

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


class AIParser:
    """–ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å NLP, ML –∏ Ollama –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self, db_session: Session, base_url: str = "https://aaa-motors.ru", 
                 ollama_model: Optional[str] = None, use_ollama: bool = True):
        self.db = db_session
        self.base_url = base_url
        self.session = None
        self.stats = {
            "total_parsed": 0,
            "total_errors": 0,
            "current_page": 0,
            "nlp_extractions": 0,
            "structure_changes_detected": 0,
            "ollama_extractions": 0
        }
        self.is_running = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama
        self.use_ollama = use_ollama
        self.ollama_model = ollama_model or getattr(settings, 'ollama_model', 'llama3:8b')
        self.ollama_working_url = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.nlp_model = self._load_nlp_model()
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ª–µ–Ω–∏–≤–æ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self.sentiment_analyzer = None
        self._sentiment_analyzer_loading = False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
        if self.use_ollama:
            self._check_ollama_availability()
        
        # –ö—ç—à –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        self.page_structure_cache = {}
        
    def _load_nlp_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç NLP –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        if not SPACY_AVAILABLE:
            logger.warning("spaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. NLP —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.")
            return None
        
        model_names = ['ru_core_news_md', 'ru_core_news_sm', 'xx_ent_wiki_sm']
        for model_name in model_names:
            try:
                nlp = spacy.load(model_name)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ NLP –º–æ–¥–µ–ª—å: {model_name}")
                return nlp
            except OSError:
                continue
        
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å NLP –º–æ–¥–µ–ª—å. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ.")
        return None
    
    def _load_sentiment_analyzer(self):
        """
        –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ - –ø–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –±–µ–∑ –Ω–µ–≥–æ
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        """
        # –ï—Å–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if self.sentiment_analyzer is not None:
            return self.sentiment_analyzer
        
        # –ï—Å–ª–∏ –∏–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (—á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
        if self._sentiment_analyzer_loading:
            return None
        
        if not TRANSFORMERS_AVAILABLE:
            return None
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∑–∞–≥—Ä—É–∑–∫–∏
        self._sentiment_analyzer_loading = True
        
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ç–∞–π–º–∞—É—Ç–∞
            import os
            os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '60'  # 1 –º–∏–Ω—É—Ç–∞ (–±—ã—Å—Ç—Ä–µ–µ)
            os.environ['HF_HUB_CACHE'] = os.path.expanduser('~/.cache/huggingface')
            
            from transformers import pipeline
            import threading
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å
            def load_analyzer():
                try:
                    logger.debug("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–≤ —Ñ–æ–Ω–µ)...")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏ DistilBERT
                    from transformers import AutoTokenizer, AutoModelForSequenceClassification
                    import torch
                    
                    model_name = "distilbert/distilbert-base-uncased-finetuned-sst-2-english"
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ pipeline –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                    
                    # –°–æ–∑–¥–∞–µ–º pipeline –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                    analyzer = pipeline(
                        "text-classification",
                        model=model,
                        tokenizer=tokenizer,
                        device=-1  # -1 –¥–ª—è CPU
                    )
                    
                    self.sentiment_analyzer = analyzer
                    logger.debug("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ DistilBERT (–≤ —Ñ–æ–Ω–µ)")
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
                    # –ü—Ä–æ–±—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ pipeline –Ω–∞–ø—Ä—è–º—É—é
                    try:
                        from transformers import pipeline
                        analyzer = pipeline(
                            "text-classification",
                            model="distilbert/distilbert-base-uncased-finetuned-sst-2-english",
                            device=-1
                        )
                        self.sentiment_analyzer = analyzer
                        logger.debug("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ pipeline")
                    except Exception as e2:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ pipeline: {e2}")
                        # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ - –ø–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –Ω–µ–≥–æ
                        self.sentiment_analyzer = None
                finally:
                    self._sentiment_analyzer_loading = False
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤ —Ñ–æ–Ω–µ
            thread = threading.Thread(target=load_analyzer, daemon=True)
            thread.start()
            
            # –ù–µ –∂–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None –ø–æ–∫–∞ –∏–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞
            return None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            self._sentiment_analyzer_loading = False
            return None
    
    def _check_ollama_availability(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–∞–±–æ—á–∏–π URL"""
        import asyncio
        from services.ollama_utils import find_working_ollama_url
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º async —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞–±–æ—á–µ–≥–æ URL
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        working_url = loop.run_until_complete(find_working_ollama_url(timeout=2.0))
        if working_url:
            self.ollama_working_url = working_url
            logger.info(f"‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω: {working_url}, –º–æ–¥–µ–ª—å: {self.ollama_model}")
            return True
        else:
            logger.warning("‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –§—É–Ω–∫—Ü–∏–∏ —Å LLM –±—É–¥—É—Ç –æ—Ç–∫–ª—é—á–µ–Ω—ã.")
            self.use_ollama = False
            return False
    
    async def _call_ollama(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Ollama –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if not self.use_ollama or not self.ollama_working_url:
            return None
        
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": self.ollama_model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                }
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.ollama_working_url}/api/chat",
                    json=payload
                )
                response.raise_for_status()
                data = response.json()
                result = data.get("message", {}).get("content", "").strip()
                
                if result:
                    self.stats["ollama_extractions"] += 1
                    return result
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Ollama: {e}")
            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π API /api/generate
            try:
                payload = {
                    "model": self.ollama_model,
                    "prompt": f"{system_prompt}\n\n{prompt}" if system_prompt else prompt,
                    "stream": False
                }
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"{self.ollama_working_url}/api/generate",
                        json=payload
                    )
                    response.raise_for_status()
                    data = response.json()
                    result = data.get("response", "").strip()
                    
                    if result:
                        self.stats["ollama_extractions"] += 1
                        return result
            except Exception as e2:
                logger.debug(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Ollama (—Å—Ç–∞—Ä—ã–π API): {e2}")
        
        return None
    
    def _call_ollama_sync(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤—ã–∑–æ–≤–∞ Ollama"""
        if not self.use_ollama or not self.ollama_working_url:
            return None
        
        try:
            # –ü—Ä–æ–±—É–µ–º –Ω–æ–≤—ã–π API /api/chat
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": self.ollama_model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                }
            }
            
            with httpx.Client(timeout=30.0) as client:
                response = client.post(
                    f"{self.ollama_working_url}/api/chat",
                    json=payload
                )
                response.raise_for_status()
                data = response.json()
                result = data.get("message", {}).get("content", "").strip()
                
                if result:
                    self.stats["ollama_extractions"] += 1
                    return result
                
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Ollama (–Ω–æ–≤—ã–π API): {e}")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π API
            try:
                payload = {
                    "model": self.ollama_model,
                    "prompt": f"{system_prompt}\n\n{prompt}" if system_prompt else prompt,
                    "stream": False
                }
                with httpx.Client(timeout=30.0) as client:
                    response = client.post(
                        f"{self.ollama_working_url}/api/generate",
                        json=payload
                    )
                    response.raise_for_status()
                    data = response.json()
                    result = data.get("response", "").strip()
                    
                    if result:
                        self.stats["ollama_extractions"] += 1
                        return result
            except Exception as e2:
                logger.debug(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Ollama: {e2}")
        
        return None
    
    def _create_session(self):
        """–°–æ–∑–¥–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        if self.session is None:
            self.session = httpx.Client(
                timeout=30.0,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1"
                },
                follow_redirects=True
            )
        return self.session
    
    def _extract_entities_nlp(self, text: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é NLP
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –¥–∞—Ç—ã, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –ª–æ–∫–∞—Ü–∏–∏, –ø—Ä–æ–¥—É–∫—Ç—ã, –¥–µ–Ω—å–≥–∏
        """
        entities = {
            "dates": [],
            "organizations": [],
            "locations": [],
            "products": [],
            "money": [],
            "other": []
        }
        
        if not self.nlp_model or not text:
            return entities
        
        try:
            doc = self.nlp_model(text)
            
            for ent in doc.ents:
                label = ent.label_.upper()
                text_clean = ent.text.strip()
                
                if label in ('DATE', 'TIME'):
                    entities["dates"].append(text_clean)
                elif label in ('ORG', 'ORGANIZATION'):
                    entities["organizations"].append(text_clean)
                elif label in ('GPE', 'LOC'):
                    entities["locations"].append(text_clean)
                elif label == 'PRODUCT':
                    entities["products"].append(text_clean)
                elif label == 'MONEY':
                    entities["money"].append(text_clean)
                else:
                    entities["other"].append(f"{label}:{text_clean}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            price_patterns = [
                r'\d+[\s,.]?\d*[\s,.]?\d*\s*‚ÇΩ',
                r'\d+[\s,.]?\d*[\s,.]?\d*\s*—Ä—É–±–ª',
                r'\d+[\s,.]?\d*[\s,.]?\d*\s*—Ä—É–±',
            ]
            for pattern in price_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                entities["money"].extend(matches)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            for key in entities:
                entities[key] = list(set(entities[key]))
            
            self.stats["nlp_extractions"] += 1
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        
        return entities
    
    def _analyze_sentiment(self, text: str) -> Optional[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ ML –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if not text:
            return None
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–µ–Ω–∏–≤–æ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω)
        if self.sentiment_analyzer is None and not self._sentiment_analyzer_loading:
            self._load_sentiment_analyzer()
        
        # –ï—Å–ª–∏ ML –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if self.sentiment_analyzer:
            try:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                text_short = text[:512]  # –ú–∞–∫—Å–∏–º—É–º 512 —Å–∏–º–≤–æ–ª–æ–≤
                result = self.sentiment_analyzer(text_short)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if isinstance(result, list) and len(result) > 0:
                    return {
                        "label": result[0].get("label", "N/A"),
                        "score": result[0].get("score", 0.0),
                        "method": "ml"
                    }
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ ML –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        
        # –ü—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ ML –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
        try:
            text_lower = text.lower()
            positive_words = ['–æ—Ç–ª–∏—á–Ω—ã–π', '—Ö–æ—Ä–æ—à–∏–π', '–Ω–æ–≤—ã–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–ø—Ä–µ–º–∏—É–º', '–∫–æ–º—Ñ–æ—Ä—Ç']
            negative_words = ['–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å', '—Ç—Ä–µ–±—É–µ—Ç', '—Ä–µ–º–æ–Ω—Ç', '–ø–æ–≤—Ä–µ–∂–¥–µ–Ω', '–∞–≤–∞—Ä–∏—è']
            
            positive_count = sum(1 for word in positive_words if word in text_lower)
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            if positive_count > negative_count:
                return {"label": "POSITIVE", "score": 0.6, "method": "heuristic"}
            elif negative_count > positive_count:
                return {"label": "NEGATIVE", "score": 0.6, "method": "heuristic"}
            else:
                return {"label": "NEUTRAL", "score": 0.5, "method": "heuristic"}
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        
        return None
    
    def _detect_structure_changes(self, url: str, soup: BeautifulSoup) -> bool:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            key_selectors = [
                soup.find('h1'),
                soup.find('title'),
                soup.find(class_=re.compile(r'price|cost', re.I)),
                soup.find(id=re.compile(r'price|cost', re.I)),
            ]
            
            structure_hash = hash(tuple(
                elem.get_text(strip=True) if elem else None
                for elem in key_selectors
            ))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            if url in self.page_structure_cache:
                if self.page_structure_cache[url] != structure_hash:
                    self.stats["structure_changes_detected"] += 1
                    logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
                    self.page_structure_cache[url] = structure_hash
                    return True
            else:
                self.page_structure_cache[url] = structure_hash
            
            return False
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
            return False
    
    def _classify_text_element(self, text: str, element_type: str = None) -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ–ø–∏—Å–∞–Ω–∏–µ, —Ü–µ–Ω–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –∏ —Ç.–¥.)
        """
        if not text:
            return "unknown"
        
        text_lower = text.lower().strip()
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º)
        if any(word in text_lower for word in ['‚ÇΩ', '—Ä—É–±', '—Ä—É–±–ª—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', 'cost', 'price']):
            return "price"
        elif any(word in text_lower for word in ['–≥–æ–¥', 'year']) and any(word in text_lower for word in ['–≤—ã–ø—É—Å–∫', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤']):
            return "year"
        elif '–≥–æ–¥ –≤—ã–ø—É—Å–∫–∞' in text_lower or '–≥–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞' in text_lower:
            return "year"
        elif any(word in text_lower for word in ['—Ç–∏–ø –∫—É–∑–æ–≤–∞', '–∫—É–∑–æ–≤', 'body']):
            return "body_type"
        elif '—Ç–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è' in text_lower or any(word in text_lower for word in ['—Ç–æ–ø–ª–∏–≤–æ', 'fuel', '–±–µ–Ω–∑–∏–Ω', '–¥–∏–∑–µ–ª—å', '–≥–∞–∑', '–≥–∏–±—Ä–∏–¥']):
            return "fuel_type"
        elif text_lower == '–∫–ø–ø' or any(word in text_lower for word in ['–∫–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á', 'transmission', 'gearbox', '–∫–æ—Ä–æ–±–∫–∞']):
            return "gear_box"
        elif '–ø—Ä–∏–≤–æ–¥' in text_lower or any(word in text_lower for word in ['drive', '–ø–æ–ª–Ω—ã–π', '–ø–µ—Ä–µ–¥–Ω–∏–π', '–∑–∞–¥–Ω–∏–π']):
            return "drive_type"
        elif '–æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è' in text_lower or ('–æ–±—ä–µ–º' in text_lower and '–¥–≤–∏–≥–∞—Ç–µ–ª—å' in text_lower):
            return "engine"
        elif '–º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è' in text_lower or ('–º–æ—â–Ω–æ—Å—Ç—å' in text_lower and '–¥–≤–∏–≥–∞—Ç–µ–ª—å' in text_lower):
            return "engine"
        elif '–ø—Ä–æ–±–µ–≥' in text_lower or any(word in text_lower for word in ['mileage', '–∫–º']):
            return "mileage"
        elif '—Ü–≤–µ—Ç' in text_lower or any(word in text_lower for word in ['color', '–æ–∫—Ä–∞—Å']):
            return "color"
        elif any(word in text_lower for word in ['–≥–æ—Ä–æ–¥', 'city', '–ª–æ–∫–∞—Ü–∏—è']):
            return "location"
        elif element_type == 'h1' or element_type == 'title':
            return "title"
        else:
            return "description"
    
    def _extract_number(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if not text:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
        numbers = re.findall(r'\d+', str(text).replace(' ', '').replace(',', '').replace('.', ''))
        if numbers:
            return int(numbers[0])
        return None
    
    def _extract_price(self, text: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP"""
        if not text:
            return None
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–µ—Ä–µ–∑ NLP
        entities = self._extract_entities_nlp(text)
        if entities.get("money"):
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —Ü–µ–Ω—É
            price_text = entities["money"][0]
            # –û—á–∏—â–∞–µ–º –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç—ã
            price_clean = re.sub(r'[^\d\s,.]', '', price_text)
            return price_clean.strip()
        
        # Fallback –Ω–∞ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        price_patterns = [
            r'(\d+[\s,.]?\d*[\s,.]?\d*)\s*‚ÇΩ',
            r'(\d+[\s,.]?\d*[\s,.]?\d*)\s*—Ä—É–±–ª',
            r'(\d+[\s,.]?\d*[\s,.]?\d*)\s*—Ä—É–±',
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None
    
    def _intelligent_extract_car_data(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ —Å–æ –°–¢–†–ê–ù–ò–¶–´ –ê–í–¢–û–ú–û–ë–ò–õ–Ø
        –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è,
        –∞ –Ω–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ. –°–µ–ª–µ–∫—Ç–æ—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø–æ–ª–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è –ø–æ URL (new/used)
        car_type = "new" if "/sale/new/" in url else "used" if "/sale/used/" in url else "unknown"
        
        car_data = {
            "source_url": url,
            "mark": None,
            "model": None,
            "city": None,
            "price": None,
            "manufacture_year": None,
            "body_type": None,
            "fuel_type": None,
            "gear_box_type": None,
            "driving_gear_type": None,
            "engine_vol": None,
            "power": None,
            "color": None,
            "mileage": None,  # –î–ª—è –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –±—É–¥–µ—Ç None
            "characteristics": {},
            "pictures": [],
            "sentiment": None,
            "nlp_entities": {},
            "ollama_extracted": {},
            "car_type": car_type  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        }
        
        logger.info(f"   üöó –¢–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {'–ù–æ–≤—ã–π' if car_type == 'new' else '–ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–π' if car_type == 'used' else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
        
        logger.debug(f"üîç –ù–∞—á–∞–ª–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è: {url}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        page_text = soup.get_text(separator=' ', strip=True)
        logger.debug(f"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(page_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if self.use_ollama and self.ollama_working_url:
            ollama_data = self._extract_with_ollama(page_text, soup)
            if ollama_data:
                car_data["ollama_extracted"] = ollama_data
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Ollama —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏
                for key, value in ollama_data.items():
                    if key in car_data and not car_data[key] and value:
                        car_data[key] = value
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ NLP
        entities = self._extract_entities_nlp(page_text)
        car_data["nlp_entities"] = entities
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π)
        description_elements = soup.find_all(['p', 'div'], class_=re.compile(r'description|about|info', re.I))
        if description_elements:
            description_text = ' '.join([elem.get_text(strip=True) for elem in description_elements[:3]])
            car_data["sentiment"] = self._analyze_sentiment(description_text)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–ª—è aaa-motors.ru (–°–¢–†–ê–ù–ò–¶–ê –ê–í–¢–û–ú–û–ë–ò–õ–Ø)
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –Ω–µ –∫–∞—Ä—Ç–æ—á–∫–µ)
        title_selectors = [
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –°–¢–†–ê–ù–ò–¶–´ –ê–í–¢–û–ú–û–ë–ò–õ–Ø (–Ω–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞)
            soup.find('h1'),  # –û–±—ã—á–Ω–æ h1 —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∞–≤—Ç–æ
            soup.find('h1', class_=re.compile(r'car-title|car-name|title', re.I)),
            soup.find('div', class_=re.compile(r'car-title|car-name|car-header', re.I)),
            soup.find('div', class_=re.compile(r'product-title|product-name', re.I)),
            # –ú–µ—Ç–∞-—Ç–µ–≥–∏ (–æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)
            soup.find('meta', property='og:title'),
            soup.find('meta', attrs={'name': 'title'}),
            soup.find('title'),
            # Fallback –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)
            soup.find('div', class_=re.compile(r'item__title|js-item-title', re.I)),
        ]
        
        logger.debug(f"üîç –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è...")
        
        title_text = None
        for idx, title_elem in enumerate(title_selectors):
            if title_elem:
                if title_elem.name == 'meta':
                    title_text = title_elem.get('content', '')
                else:
                    title_text = title_elem.get_text(strip=True)
                if title_text and len(title_text) > 3:
                    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    title_text = re.sub(r'\s+', ' ', title_text).strip()
                    logger.debug(f"   ‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä #{idx}: {title_text[:100]}")
                    break
        
        if not title_text:
            logger.warning(f"   ‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–Ω–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º")
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if not title_text:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "Daewoo Matiz" –∏–ª–∏ "BMW X5" –≤ —Ç–µ–∫—Å—Ç–µ
            title_pattern = re.search(r'([A-Z–ê-–Ø][a-z–∞-—è]+(?:\s+[A-Z–ê-–Ø][a-z–∞-—è]+)+)', page_text[:500])
            if title_pattern:
                title_text = title_pattern.group(1)
        
        if title_text:
            # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤
            title_text = re.sub(r'\s+(–≤ –Ω–∞–ª–∏—á–∏–∏|–∫—É–ø–∏—Ç—å|–ø—Ä–æ–¥–∞–∂–∞|–∞–≤—Ç–æ–º–æ–±–∏–ª—å|–ø—Ä–æ–¥–∞–º|–∫—É–ø–∏—Ç—å|—Ü–µ–Ω–∞)', '', title_text, flags=re.I).strip()
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–¥–ª—è —Å–ª—É—á–∞–µ–≤ —Ç–∏–ø–∞ "–ú–æ—Å–∫–≤–∏—á –ú–û–°–ö–í–ò–ß 3" –∏–ª–∏ "Daewoo Daewoo Matiz")
            words = title_text.split()
            unique_words = []
            seen_lower = set()
            for word in words:
                word_clean = word.strip()
                if word_clean and len(word_clean) > 1:
                    word_lower = word_clean.lower()
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º)
                    if word_lower not in seen_lower or word_clean.isdigit():
                        unique_words.append(word_clean)
                        if not word_clean.isdigit():
                            seen_lower.add(word_lower)
            
            title_text_clean = " ".join(unique_words)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º NLP –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ä–∫–∏ –∏ –º–æ–¥–µ–ª–∏
            title_entities = self._extract_entities_nlp(title_text_clean)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            # –§–æ—Ä–º–∞—Ç –æ–±—ã—á–Ω–æ: "Daewoo Matiz" –∏–ª–∏ "BMW X5" –∏–ª–∏ "–ú–æ—Å–∫–≤–∏—á –ú–û–°–ö–í–ò–ß 3"
            title_parts = [p for p in title_text_clean.split() if len(p) > 1]  # –£–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            
            if len(title_parts) >= 2:
                # –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –æ–±—ã—á–Ω–æ –º–∞—Ä–∫–∞
                car_data["mark"] = title_parts[0]
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ - –º–æ–¥–µ–ª—å (–Ω–æ –Ω–µ –±–æ–ª–µ–µ 3 —Å–ª–æ–≤)
                car_data["model"] = " ".join(title_parts[1:min(4, len(title_parts))])
                logger.debug(f"   ‚úÖ –ú–∞—Ä–∫–∞ –∏ –º–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω—ã: {car_data['mark']} {car_data['model']}")
            elif len(title_parts) == 1:
                # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞—Ä–∫–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å
                car_data["mark"] = title_parts[0]
                logger.debug(f"   ‚ö†Ô∏è –¢–æ–ª—å–∫–æ –º–∞—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞: {car_data['mark']}")
                
                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –º–æ–¥–µ–ª—å –∏–∑ URL
                url_parts = url.rstrip('/').split('/')
                if len(url_parts) >= 5:
                    potential_model = url_parts[-2].replace('-', ' ').title()
                    if potential_model and potential_model.lower() != car_data["mark"].lower():
                        car_data["model"] = potential_model
                        logger.debug(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['model']}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º NLP —Å—É—â–Ω–æ—Å—Ç–∏ (PRODUCT)
            if entities.get("products"):
                for product in entities["products"]:
                    product_clean = re.sub(r'[^\w\s]', '', product).strip()
                    parts = [p for p in product_clean.split() if len(p) > 1]
                    if len(parts) >= 2:
                        if not car_data["mark"]:
                            car_data["mark"] = parts[0]
                            logger.debug(f"   ‚úÖ –ú–∞—Ä–∫–∞ –∏–∑ NLP: {car_data['mark']}")
                        if not car_data["model"]:
                            car_data["model"] = " ".join(parts[1:min(4, len(parts))])
                            logger.debug(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∏–∑ NLP: {car_data['model']}")
        else:
            logger.warning(f"   ‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–∞—Ä–∫–∞ –∏ –º–æ–¥–µ–ª—å –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
            
            # Fallback: –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ URL
            url_parts = url.rstrip('/').split('/')
            if len(url_parts) >= 5:
                if not car_data.get("mark"):
                    car_data["mark"] = url_parts[-3].replace('-', ' ').title()
                    logger.info(f"   ‚úÖ –ú–∞—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['mark']}")
                if not car_data.get("model"):
                    car_data["model"] = url_parts[-2].replace('-', ' ').title()
                    logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['model']}")
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫)
        price_candidates = []
        
        # –ú–µ—Ç–æ–¥ 1: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è)
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: card-info__price-main (–æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è)
        logger.debug("üîç –ü–æ–∏—Å–∫ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ card-info__price-main...")
        card_info_price = soup.find('div', class_='card-info__price-main')
        if not card_info_price:
            # –ü—Ä–æ–±—É–µ–º —Å regex
            card_info_price = soup.find('div', class_=re.compile(r'card-info__price-main', re.I))
        if card_info_price:
            price_text = card_info_price.get_text(strip=True)
            if price_text:
                logger.info(f"   ‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ card-info__price-main: {price_text}")
                print(f"   ‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞: {price_text}")
                price_candidates.append(price_text)
            else:
                logger.debug(f"   ‚ö†Ô∏è card-info__price-main –Ω–∞–π–¥–µ–Ω, –Ω–æ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π")
        else:
            logger.debug(f"   ‚ö†Ô∏è card-info__price-main –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: card-info__price (–±–ª–æ–∫ —Å —Ü–µ–Ω–æ–π)
        logger.debug("üîç –ü–æ–∏—Å–∫ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ card-info__price...")
        card_info_price_block = soup.find('div', class_='card-info__price')
        if not card_info_price_block:
            card_info_price_block = soup.find('div', class_=re.compile(r'card-info__price', re.I))
        if card_info_price_block:
            price_elem = card_info_price_block.find('div', class_='card-info__price-main')
            if not price_elem:
                price_elem = card_info_price_block.find('div', class_=re.compile(r'card-info__price-main|price-main', re.I))
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                if price_text and price_text not in price_candidates:
                    logger.info(f"   ‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ card-info__price: {price_text}")
                    print(f"   ‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞: {price_text}")
                    price_candidates.append(price_text)
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ card-info__price
                price_text = card_info_price_block.get_text(strip=True)
                # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ç–µ–∫—Å—Ç–µ
                price_match = re.search(r'(\d{1,3}(?:\s+\d{3})+)\s*[—Ä—Ä]\.?', price_text)
                if price_match:
                    price_found = price_match.group(1).replace(' ', '') + ' —Ä.'
                    if price_found not in price_candidates:
                        logger.info(f"   ‚úÖ –¶–µ–Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ card-info__price: {price_found}")
                        print(f"   ‚úÖ –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞: {price_found}")
                        price_candidates.append(price_found)
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: card__aside-block –∏–ª–∏ card__main - –æ—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏ —Å —Ü–µ–Ω–æ–π
        price_blocks = [
            soup.find('div', class_=re.compile(r'card__aside-block|card__main|card__header', re.I)),
            soup.find('div', class_=re.compile(r'card__price|card-price', re.I)),
        ]
        
        for block in price_blocks:
            if block:
                # –ò—â–µ–º —Ü–µ–Ω—É –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞
                price_elem = block.find('div', class_=re.compile(r'price|cost', re.I))
                if not price_elem:
                    price_elem = block.find('span', class_=re.compile(r'price|cost', re.I))
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    if price_text and price_text not in price_candidates:
                        price_candidates.append(price_text)
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 4: –í—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ price
        price_selectors = [
            soup.find_all('div', class_=re.compile(r'card__price|card-price|item__price-main|price-main|item__price', re.I)),
            soup.find_all('span', class_=re.compile(r'card__price|price|cost|—Å—Ç–æ–∏–º–æ—Å—Ç—å', re.I)),
            soup.find_all('div', class_=re.compile(r'price|cost|—Å—Ç–æ–∏–º–æ—Å—Ç—å', re.I)),
            soup.find_all(class_=re.compile(r'price|cost|—Å—Ç–æ–∏–º–æ—Å—Ç—å', re.I)),
            soup.find_all(id=re.compile(r'price|cost', re.I)),
        ]
        
        for selector_list in price_selectors:
            for elem in selector_list:
                price_text = elem.get_text(strip=True)
                if price_text and price_text not in price_candidates:
                    price_candidates.append(price_text)
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ NLP (–¥–µ–Ω—å–≥–∏)
        if entities.get("money"):
            price_candidates.extend(entities["money"])
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π)
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã: "217 000 —Ä.", "217000 —Ä—É–±", "217000‚ÇΩ", "217 000 ‚ÇΩ"
        price_patterns = [
            re.search(r'(\d{1,3}(?:\s+\d{3})+)\s*[—Ä—Ä]\.?', page_text),
            re.search(r'(\d{1,3}(?:\s+\d{3})+)\s*—Ä—É–±', page_text, re.I),
            re.search(r'(\d{1,3}(?:\s+\d{3})+)\s*‚ÇΩ', page_text),
            re.search(r'‚ÇΩ\s*(\d{1,3}(?:\s+\d{3})+)', page_text),
            re.search(r'(\d{1,3}(?:\s+\d{3})+)\s*‚ÇΩ', page_text),
            # –¢–∞–∫–∂–µ –∏—â–µ–º —á–∏—Å–ª–∞ –æ—Ç 100000 –¥–æ 10000000 (—Ä–∞–∑—É–º–Ω—ã–µ —Ü–µ–Ω—ã)
            re.search(r'(\d{6,7})\s*(?:—Ä—É–±|—Ä\.?|‚ÇΩ)', page_text, re.I),
        ]
        
        for pattern in price_patterns:
            if pattern:
                price_text = pattern.group(1).replace(' ', '') + ' —Ä.'
                if price_text not in price_candidates:
                    price_candidates.append(price_text)
        
        # –ú–µ—Ç–æ–¥ 3.5: –ü–æ–∏—Å–∫ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (data-–∞—Ç—Ä–∏–±—É—Ç—ã, –º–µ—Ç–∞-—Ç–µ–≥–∏)
        meta_price = soup.find('meta', property=re.compile(r'price|cost', re.I))
        if meta_price:
            price_content = meta_price.get('content', '')
            if price_content:
                price_candidates.append(price_content)
        
        # –ú–µ—Ç–æ–¥ 4: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã
        price_text = self._extract_price(page_text)
        if price_text:
            price_candidates.append(price_text)
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é —Ü–µ–Ω—É (—Å–∞–º—É—é –±–æ–ª—å—à—É—é —á–∏—Å–ª–æ–≤—É—é)
        best_price = None
        best_price_value = 0
        logger.debug(f"   üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(price_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Ü–µ–Ω—É...")
        
        for candidate in price_candidates:
            logger.debug(f"      –ö–∞–Ω–¥–∏–¥–∞—Ç: {candidate}")
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–µ—Ä–µ–∑ _extract_price
            extracted_price = self._extract_price(candidate)
            if not extracted_price:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –Ω–∞–ø—Ä—è–º—É—é
                # –§–æ—Ä–º–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å "455 000 —Ä." –∏–ª–∏ "455000 —Ä."
                price_num_str = re.sub(r'[^\d]', '', candidate)
                if price_num_str and len(price_num_str) >= 5:  # –ú–∏–Ω–∏–º—É–º 5 —Ü–∏—Ñ—Ä –¥–ª—è —Ü–µ–Ω—ã
                    extracted_price = price_num_str
                    logger.debug(f"      –ò–∑–≤–ª–µ—á–µ–Ω–æ —á–∏—Å–ª–æ –Ω–∞–ø—Ä—è–º—É—é: {extracted_price}")
            
            if extracted_price:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                price_num = self._extract_number(extracted_price)
                if price_num and price_num > best_price_value:
                    best_price_value = price_num
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—É –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä "455 000 —Ä.")
                    if price_num >= 1000:
                        best_price = f"{price_num:,}".replace(',', ' ') + ' —Ä.'
                    else:
                        best_price = str(price_num) + ' —Ä.'
                    logger.info(f"   ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è —Ü–µ–Ω–∞: {best_price} (–∑–Ω–∞—á–µ–Ω–∏–µ: {best_price_value})")
        
        if best_price:
            car_data["price"] = best_price
            logger.info(f"   ‚úÖ –¶–µ–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {best_price}")
            print(f"   ‚úÖ –¶–µ–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {best_price}")
        else:
            logger.warning(f"   ‚ö†Ô∏è –¶–µ–Ω–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ {len(price_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            if price_candidates:
                logger.warning(f"      –ö–∞–Ω–¥–∏–¥–∞—Ç—ã: {price_candidates}")
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru
        city_selectors = [
            soup.find('div', class_=re.compile(r'item-row__info-address|address|city', re.I)),
            soup.find('span', class_=re.compile(r'address|city|location', re.I)),
        ]
        
        for city_elem in city_selectors:
            if city_elem:
                city_text = city_elem.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ –∞–¥—Ä–µ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ê–°–ü –ê–ê–ê –ú–æ—Ç–æ—Ä—Å –†–æ—Å—Ç–æ–≤ –¢–µ–∫—É—á–µ–≤–∞ 352–ë" -> "–†–æ—Å—Ç–æ–≤")
                if city_text:
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
                    city_parts = city_text.split()
                    for part in city_parts:
                        if len(part) > 3 and not part[0].isdigit():
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –≥–æ—Ä–æ–¥–∞
                            if any(word in part.lower() for word in ['–º–æ—Å–∫–≤', '—Ä–æ—Å—Ç–æ–≤', '—Å–ø–±', '–ø–∏—Ç–µ—Ä', '–∫–∞–∑–∞–Ω', '–Ω–∏–∂–Ω', '–Ω–æ–≤–æ—Å–∏–±', '–µ–∫–∞—Ç–µ—Ä–∏–Ω']):
                                car_data["city"] = part
                                break
                    if not car_data["city"] and city_text:
                        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ
                        city_clean = re.sub(r'[,;]\s*$', '', city_text).strip()
                        car_data["city"] = city_clean
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º NLP
        if not car_data["city"] and entities.get("locations"):
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ª–æ–∫–∞—Ü–∏—é –∫–∞–∫ –≥–æ—Ä–æ–¥ –∏ –æ—á–∏—â–∞–µ–º
            city_from_entities = entities["locations"][0]
            if city_from_entities:
                car_data["city"] = re.sub(r'[,;]\s*$', '', city_from_entities).strip()
        
        # –û—á–∏—Å—Ç–∫–∞ –≥–æ—Ä–æ–¥–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if car_data["city"]:
            car_data["city"] = re.sub(r'[,;]\s*$', '', car_data["city"]).strip()
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru
        year_selectors = [
            # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É "–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞: 2009"
            soup.find_all(string=re.compile(r'–ì–æ–¥\s+–≤—ã–ø—É—Å–∫–∞|–ì–æ–¥\s+–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞', re.I)),
        ]
        
        for year_elem in year_selectors:
            if year_elem:
                # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç —Å –≥–æ–¥–æ–º
                parent = year_elem.parent if hasattr(year_elem, 'parent') else None
                if parent:
                    parent_text = parent.get_text(strip=True)
                    year = self._extract_number(parent_text)
                    if year and 1900 <= year <= 2100 and not car_data["manufacture_year"]:
                        car_data["manufacture_year"] = year
                        break
        
        # Fallback –Ω–∞ NLP –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        if not car_data["manufacture_year"] and entities.get("dates"):
            for date_text in entities["dates"]:
                year = self._extract_number(date_text)
                if year and 1900 <= year <= 2100:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                    car_data["manufacture_year"] = year
                    break
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: card__tech (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è) - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ <div><span>–ù–∞–∑–≤–∞–Ω–∏–µ</span><span>–ó–Ω–∞—á–µ–Ω–∏–µ</span></div>
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ –¥–ª—è –Ω–æ–≤—ã—Ö, —Ç–∞–∫ –∏ –¥–ª—è –ø–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        card_tech = soup.find('div', class_=re.compile(r'card__tech|js-card-tech', re.I))
        if card_tech:
            logger.debug("üîç –ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ card__tech, –ø–∞—Ä—Å–∏–Ω–≥ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫...")
            # –ò—â–µ–º –≤—Å–µ –¥–æ—á–µ—Ä–Ω–∏–µ div —ç–ª–µ–º–µ–Ω—Ç—ã
            tech_items = card_tech.find_all('div', recursive=False)
            logger.debug(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(tech_items)}")
            
            for tech_item in tech_items:
                # –ò—â–µ–º –¥–≤–∞ span —ç–ª–µ–º–µ–Ω—Ç–∞: –ø–µ—Ä–≤—ã–π - –Ω–∞–∑–≤–∞–Ω–∏–µ, –≤—Ç–æ—Ä–æ–π - –∑–Ω–∞—á–µ–Ω–∏–µ
                spans = tech_item.find_all('span', recursive=False)
                if len(spans) >= 2:
                    key = spans[0].get_text(strip=True)
                    value = spans[1].get_text(strip=True)
                    key_lower = key.lower()
                    
                    logger.debug(f"   üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ: {key} = {value}")
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                    self._parse_characteristic(key, value, car_data)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ JSON
                    if not car_data.get("characteristics"):
                        car_data["characteristics"] = {}
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –≤–∫–ª—é—á–∞—è –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                    standard_fields = ['–≥–æ–¥ –≤—ã–ø—É—Å–∫–∞', '–≥–æ–¥', '–æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è', '–æ–±—ä–µ–º', 
                                      '—Ç–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è', '–º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è', '–º–æ—â–Ω–æ—Å—Ç—å',
                                      '–ø—Ä–æ–±–µ–≥', '–ø—Ä–∏–≤–æ–¥', '–∫–ø–ø', '—Ü–≤–µ—Ç', '—Ä—É–ª—å', '—Ç–∏–ø –∫—É–∑–æ–≤–∞',
                                      '–∫—É–∑–æ–≤', '–º–∞–∫—Å —Å–∫–æ—Ä–æ—Å—Ç—å', '–≤–µ—Å']
                    
                    if key_lower not in standard_fields:
                        # –≠—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–º–∞–∫—Å —Å–∫–æ—Ä–æ—Å—Ç—å, –≤–µ—Å –∏ —Ç.–¥.)
                        car_data["characteristics"][key] = value
                    elif key_lower in ['–º–∞–∫—Å —Å–∫–æ—Ä–æ—Å—Ç—å', '–≤–µ—Å']:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–∫—Å —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –≤–µ—Å –≤ characteristics
                        car_data["characteristics"][key] = value
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏ (card__com-wrap)
        card_com = soup.find('div', class_=re.compile(r'card__com-wrap|js-card-com', re.I))
        if card_com:
            logger.debug("üîç –ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏ (card__com-wrap), –ø–∞—Ä—Å–∏–Ω–≥ –æ–ø—Ü–∏–π...")
            
            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ –æ–ø—Ü–∏–π
            com_items = card_com.find_all('div', class_=re.compile(r'card__com-item|com-item', re.I))
            all_options = []
            
            for com_item in com_items:
                # –ò—â–µ–º ul —Å–ø–∏—Å–∫–∏ –≤–Ω—É—Ç—Ä–∏
                ul_lists = com_item.find_all('ul')
                for ul in ul_lists:
                    # –ò—â–µ–º –≤—Å–µ li —ç–ª–µ–º–µ–Ω—Ç—ã
                    li_items = ul.find_all('li')
                    for li in li_items:
                        option_text = li.get_text(strip=True)
                        if option_text and len(option_text) > 3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–ø—Ü–∏–∏
                            all_options.append(option_text)
            
            if all_options:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—é –≤ characteristics
                if not car_data.get("characteristics"):
                    car_data["characteristics"] = {}
                
                car_data["characteristics"]["equipment"] = all_options
                car_data["characteristics"]["equipment_count"] = len(all_options)
                logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –æ–ø—Ü–∏–π –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏: {len(all_options)}")
                logger.debug(f"   üìã –ü–µ—Ä–≤—ã–µ 5 –æ–ø—Ü–∏–π: {all_options[:5]}")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –î—Ä—É–≥–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        spec_containers = [
            # –î–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è - –∏—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
            soup.find_all('div', class_=re.compile(r'spec|characteristic|param|feature|car-info', re.I)),
            # –î–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
            soup.find_all('div', class_=re.compile(r'item__tech|tech', re.I)),
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            soup.find_all('table'),
            soup.find_all('dl'),
            soup.find_all('ul', class_=re.compile(r'spec|list|feature', re.I)),
        ]
        
        for container_list in spec_containers:
            for container in container_list:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä—ã –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
                if container.name == 'table':
                    rows = container.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            key = cells[0].get_text(strip=True).lower()
                            value = cells[1].get_text(strip=True)
                            self._parse_characteristic(key, value, car_data)
                
                elif container.name == 'dl':
                    dts = container.find_all('dt')
                    dds = container.find_all('dd')
                    for dt, dd in zip(dts, dds):
                        key = dt.get_text(strip=True).lower()
                        value = dd.get_text(strip=True)
                        self._parse_characteristic(key, value, car_data)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ "–ù–∞–∑–≤–∞–Ω–∏–µ: –ó–Ω–∞—á–µ–Ω–∏–µ"
                # –≠—Ç–æ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –≥–¥–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
                if container.name in ['div', 'section', 'article']:
                    # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –≤–∏–¥–∞ "–ù–∞–∑–≤–∞–Ω–∏–µ: –ó–Ω–∞—á–µ–Ω–∏–µ"
                    text_elements = container.find_all(string=re.compile(r'[–ê-–Ø–∞-—è]+\s*:', re.I))
                    for text_elem in text_elements:
                        if text_elem.parent:
                            full_text = text_elem.parent.get_text(strip=True)
                            if ':' in full_text:
                                parts = full_text.split(':', 1)
                                if len(parts) == 2:
                                    key = parts[0].strip().lower()
                                    value = parts[1].strip()
                                    self._parse_characteristic(key, value, car_data)
                
                elif container.name in ['div', 'ul']:
                    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è aaa-motors.ru
                    # –í item__tech –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã <div> —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
                    # –§–æ—Ä–º–∞—Ç HTML: <div>2009 –≥.<span>\</span></div>
                    items = container.find_all(['li', 'div'], recursive=False)
                    if not items:
                        # –ï—Å–ª–∏ –ø—Ä—è–º—ã—Ö –¥–æ—á–µ—Ä–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        items = container.find_all(['li', 'div', 'span'])
                    
                    for item in items:
                        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –¥–æ—á–µ—Ä–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                        text = ''
                        for child in item.children:
                            if hasattr(child, 'string') and child.string:
                                text += child.string
                            elif hasattr(child, 'get_text'):
                                child_text = child.get_text(strip=True)
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç–∏–ø–∞ "\"
                                if child_text and child_text not in ['\\', '|', '/']:
                                    text += ' ' + child_text
                        
                        if not text:
                            text = item.get_text(strip=True)
                        
                        # –î–ª—è aaa-motors.ru —Ñ–æ—Ä–º–∞—Ç: "2009 –≥.\" –∏–ª–∏ "145 000 –∫–º\" –∏–ª–∏ "–ë–µ–Ω–∑–∏–Ω\"
                        # –£–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–µ—à –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                        text = re.sub(r'[\\|/]', '', text).strip()
                        
                        if not text or len(text) < 2:
                            continue
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω "–∫–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ"
                        if ':' in text:
                            parts = text.split(':', 1)
                            if len(parts) == 2:
                                key = parts[0].strip().lower()
                                value = parts[1].strip()
                                self._parse_characteristic(key, value, car_data)
                        else:
                            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                            # –§–æ—Ä–º–∞—Ç: "2009 –≥." -> –≥–æ–¥
                            if re.search(r'\d{4}\s*–≥\.?', text):
                                year = self._extract_number(text)
                                if year and 1900 <= year <= 2100 and not car_data["manufacture_year"]:
                                    car_data["manufacture_year"] = year
                            # –§–æ—Ä–º–∞—Ç: "145 000 –∫–º" -> –ø—Ä–æ–±–µ–≥
                            elif '–∫–º' in text.lower():
                                mileage = self._extract_number(text)
                                if mileage and not car_data["mileage"]:
                                    car_data["mileage"] = mileage
                            # –§–æ—Ä–º–∞—Ç: "0.8 –ª" -> –æ–±—ä–µ–º
                            elif re.search(r'\d+\.?\d*\s*–ª\b', text, re.I):
                                vol_text = re.search(r'(\d+\.?\d*)', text)
                                if vol_text:
                                    try:
                                        vol = float(vol_text.group(1))
                                        if not car_data["engine_vol"]:
                                            car_data["engine_vol"] = int(vol)
                                    except:
                                        pass
                            # –§–æ—Ä–º–∞—Ç: "52 –ª.—Å." -> –º–æ—â–Ω–æ—Å—Ç—å
                            elif '–ª.—Å.' in text.lower() or '–ª—Å' in text.lower():
                                power_text = text.replace('–ª.—Å.', '').replace('–ª—Å', '').strip()
                                if not car_data["power"]:
                                    car_data["power"] = power_text
                            # –ë–µ–Ω–∑–∏–Ω, –î–∏–∑–µ–ª—å -> —Ç–æ–ø–ª–∏–≤–æ
                            elif any(word in text.lower() for word in ['–±–µ–Ω–∑–∏–Ω', '–¥–∏–∑–µ–ª—å', '—ç–ª–µ–∫—Ç—Ä–æ', '–≥–∏–±—Ä–∏–¥']):
                                if not car_data["fuel_type"]:
                                    car_data["fuel_type"] = text
                            # –ú–µ—Ö–∞–Ω–∏–∫–∞, –ê–≤—Ç–æ–º–∞—Ç -> –∫–æ—Ä–æ–±–∫–∞
                            elif any(word in text.lower() for word in ['–º–µ—Ö–∞–Ω–∏–∫', '–∞–≤—Ç–æ–º–∞—Ç', '–≤–∞—Ä–∏–∞—Ç–æ—Ä', '—Ä–æ–±–æ—Ç']):
                                if not car_data["gear_box_type"]:
                                    car_data["gear_box_type"] = text
                            # –ü—Ä–∏–≤–æ–¥: –ü–µ—Ä–µ–¥–Ω–∏–π, –ó–∞–¥–Ω–∏–π, –ü–æ–ª–Ω—ã–π
                            elif any(word in text.lower() for word in ['–ø–µ—Ä–µ–¥–Ω–∏–π', '–∑–∞–¥–Ω–∏–π', '–ø–æ–ª–Ω—ã–π', '4wd', 'awd']):
                                if not car_data["driving_gear_type"]:
                                    car_data["driving_gear_type"] = text
                            # –¢–∏–ø –∫—É–∑–æ–≤–∞: –•–µ—Ç—á–±—ç–∫, –°–µ–¥–∞–Ω, –∏ —Ç.–¥.
                            elif any(word in text.lower() for word in ['—Ö–µ—Ç—á–±—ç–∫', '—Å–µ–¥–∞–Ω', '—É–Ω–∏–≤–µ—Ä—Å–∞–ª', '–∫—É–ø–µ', '–≤–Ω–µ–¥–æ—Ä–æ–∂–Ω–∏–∫', 'suv', '–∫—Ä–æ—Å—Å–æ–≤–µ—Ä']):
                                if not car_data["body_type"]:
                                    car_data["body_type"] = text
                            # –¶–≤–µ—Ç (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —è–≤–Ω–æ)
                            elif any(word in text.lower() for word in ['—Å–∏–Ω–∏–π', '–∫—Ä–∞—Å–Ω—ã–π', '—á–µ—Ä–Ω—ã–π', '–±–µ–ª—ã–π', '—Å–µ—Ä—ã–π', '–∑–µ–ª–µ–Ω—ã–π']):
                                if not car_data["color"]:
                                    car_data["color"] = text
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç data-src –∏ lozad)
        img_selectors = [
            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è aaa-motors.ru (lozad lazy loading)
            soup.find_all('img', class_=re.compile(r'lozad|item-row__img', re.I)),
            soup.find_all('img', {'data-src': True}),
            soup.find_all('img', class_=re.compile(r'car|photo|image|gallery|auto', re.I)),
            soup.find_all('img', src=re.compile(r'car|auto|photo|image|media\.cm\.expert', re.I)),
            soup.find_all('img', alt=re.compile(r'car|auto|–º–∞—à–∏–Ω|–∞–≤—Ç–æ–º–æ–±–∏–ª', re.I)),
        ]
        
        all_images = set()
        for selector_list in img_selectors:
            for img in selector_list:
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: data-src (–¥–ª—è lazy loading), –∑–∞—Ç–µ–º src
                src = img.get('data-src') or img.get('src') or img.get('data-lazy-src') or img.get('data-original')
                if src:
                    # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
                    if '?' in src:
                        src = src.split('?')[0]
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = urljoin(self.base_url, src)
                    elif not src.startswith('http'):
                        src = urljoin(self.base_url, src)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º placeholder –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if 'placeholder' not in src.lower() and 'no-image' not in src.lower():
                        all_images.add(src)
        
        sorted_images = sorted(list(all_images))
        for idx, img_url in enumerate(sorted_images[:20]):
            car_data["pictures"].append({
                "image_url": img_url,
                "seqno": idx
            })
        
        return car_data
    
    def _extract_with_ollama(self, page_text: str, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ —Å –ø–æ–º–æ—â—å—é Ollama LLM
        """
        if not self.use_ollama or not self.ollama_working_url:
            return None
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Ollama
        system_prompt = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª—è—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞. 
–ò–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π null."""
        
        user_prompt = f"""–ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "mark": "–º–∞—Ä–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–ª–∏ null",
  "model": "–º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–ª–∏ null",
  "city": "–≥–æ—Ä–æ–¥ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ null",
  "price": "—Ü–µ–Ω–∞ –≤ —Ä—É–±–ª—è—Ö (—Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã) –∏–ª–∏ null",
  "manufacture_year": "–≥–æ–¥ –≤—ã–ø—É—Å–∫–∞ (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ) –∏–ª–∏ null",
  "body_type": "—Ç–∏–ø –∫—É–∑–æ–≤–∞ –∏–ª–∏ null",
  "fuel_type": "—Ç–∏–ø —Ç–æ–ø–ª–∏–≤–∞ –∏–ª–∏ null",
  "gear_box_type": "—Ç–∏–ø –∫–æ—Ä–æ–±–∫–∏ –ø–µ—Ä–µ–¥–∞—á –∏–ª–∏ null",
  "driving_gear_type": "—Ç–∏–ø –ø—Ä–∏–≤–æ–¥–∞ –∏–ª–∏ null",
  "engine_vol": "–æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è –≤ –ª–∏—Ç—Ä–∞—Ö (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ) –∏–ª–∏ null",
  "power": "–º–æ—â–Ω–æ—Å—Ç—å –≤ –ª.—Å. –∏–ª–∏ null",
  "color": "—Ü–≤–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–ª–∏ null",
  "mileage": "–ø—Ä–æ–±–µ–≥ –≤ –∫–º (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ) –∏–ª–∏ null"
}}

–¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã:
{page_text[:3000]}"""
        
        try:
            result = self._call_ollama_sync(user_prompt, system_prompt)
            if result:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)
                # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç, –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–π {
                start_idx = result.find('{')
                if start_idx != -1:
                    # –ò—â–µ–º –ø–∞—Ä–Ω—É—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                    bracket_count = 0
                    end_idx = start_idx
                    for i in range(start_idx, len(result)):
                        if result[i] == '{':
                            bracket_count += 1
                        elif result[i] == '}':
                            bracket_count -= 1
                            if bracket_count == 0:
                                end_idx = i + 1
                                break
                    
                    if bracket_count == 0:
                        json_str = result[start_idx:end_idx]
                        try:
                            extracted_data = json.loads(json_str)
                            logger.debug(f"‚úÖ Ollama –∏–∑–≤–ª–µ–∫ –¥–∞–Ω–Ω—ã–µ: {len(extracted_data)} –ø–æ–ª–µ–π")
                            return extracted_data
                        except json.JSONDecodeError as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Ollama: {e}")
                            logger.debug(f"–û—Ç–≤–µ—Ç Ollama: {result[:200]}")
                    else:
                        logger.debug(f"–ù–µ–ø–æ–ª–Ω—ã–π JSON –≤ –æ—Ç–≤–µ—Ç–µ Ollama: {result[:200]}")
                else:
                    logger.debug(f"JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ Ollama: {result[:200]}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Ollama: {e}")
        
        return None
    
    def _parse_characteristic(self, key: str, value: str, car_data: Dict[str, Any]):
        """–ü–∞—Ä—Å–∏—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        classification = self._classify_text_element(key)
        
        if classification == "price" and not car_data["price"]:
            car_data["price"] = self._extract_price(value)
        elif classification == "year" and not car_data["manufacture_year"]:
            year = self._extract_number(value)
            if year and 1900 <= year <= 2100:
                car_data["manufacture_year"] = year
        elif classification == "body_type" and not car_data["body_type"]:
            car_data["body_type"] = value
        elif classification == "fuel_type" and not car_data["fuel_type"]:
            car_data["fuel_type"] = value
        elif classification == "gear_box" and not car_data["gear_box_type"]:
            car_data["gear_box_type"] = value
        elif classification == "drive_type" and not car_data["driving_gear_type"]:
            car_data["driving_gear_type"] = value
        elif classification == "engine_volume":
            # –≠—Ç–æ –æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è
            if '–æ–±—ä–µ–º' in key.lower() or '–æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è' in key.lower() or ('–ª' in value.lower() and '–ª.—Å.' not in value.lower() and '–ª—Å' not in value.lower()):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä–µ–º –≤ –ª–∏—Ç—Ä–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: "0.8 –ª" -> 800, "1.0 –ª" -> 1000, "1.6 –ª" -> 1600)
                vol_text = value.lower().replace('–ª', '').replace('–ª.', '').replace('–ª–∏—Ç—Ä', '').replace('–ª–∏—Ç—Ä–æ–≤', '').strip()
                
                # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ
                vol_text = vol_text.replace(' ', '').replace(',', '.')
                
                try:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ —Å —Ç–æ—á–∫–æ–π –∏–ª–∏ –±–µ–∑
                    if '.' in vol_text:
                        vol_float = float(vol_text)
                        # –ï—Å–ª–∏ —á–∏—Å–ª–æ –º–µ–Ω—å—à–µ 10, —ç—Ç–æ –ª–∏—Ç—Ä—ã - —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1000
                        # –ï—Å–ª–∏ —á–∏—Å–ª–æ –±–æ–ª—å—à–µ 10, —ç—Ç–æ —É–∂–µ –º–∏–ª–ª–∏–ª–∏—Ç—Ä—ã
                        if vol_float < 10:
                            vol = int(vol_float * 1000)  # 0.8 –ª = 800 –º–ª, 1.0 –ª = 1000 –º–ª, 1.6 –ª = 1600 –º–ª
                        else:
                            vol = int(vol_float)  # –£–∂–µ –≤ –º–∏–ª–ª–∏–ª–∏—Ç—Ä–∞—Ö
                    else:
                        # –¶–µ–ª–æ–µ —á–∏—Å–ª–æ
                        vol = int(vol_text)
                        # –ï—Å–ª–∏ —á–∏—Å–ª–æ –º–µ–Ω—å—à–µ 10, —ç—Ç–æ –ª–∏—Ç—Ä—ã - —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1000
                        if vol < 10:
                            vol = vol * 1000  # 1 –ª = 1000 –º–ª
                        # –ï—Å–ª–∏ —á–∏—Å–ª–æ –±–æ–ª—å—à–µ 10000, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —É–∂–µ –≤ –º–∏–ª–ª–∏–ª–∏—Ç—Ä–∞—Ö –±–µ–∑ –∑–∞–ø—è—Ç–æ–π
                        # –ù–æ –æ–±—ã—á–Ω–æ –æ–±—ä–µ–º—ã –¥–æ 10 –ª–∏—Ç—Ä–æ–≤, —Ç–∞–∫ —á—Ç–æ —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1000 –µ—Å–ª–∏ –º–µ–Ω—å—à–µ 10
                except (ValueError, AttributeError):
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ _extract_number
                    vol = self._extract_number(value)
                    # –ï—Å–ª–∏ —á–∏—Å–ª–æ –º–µ–Ω—å—à–µ 10, —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 1000
                    if vol and vol < 10:
                        vol = vol * 1000
                
                if vol and not car_data["engine_vol"]:
                    car_data["engine_vol"] = vol
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                    if vol >= 1000:
                        vol_litres = vol / 1000.0
                        logger.debug(f"   ‚úÖ –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è: {vol} –º–ª ({vol_litres:.1f} –ª)")
                    else:
                        logger.debug(f"   ‚úÖ –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è: {vol} –º–ª")
        elif classification == "engine_power":
            # –≠—Ç–æ –º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è
            if '–º–æ—â–Ω–æ—Å—Ç—å' in key.lower() or '–º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è' in key.lower() or '–ª.—Å.' in value.lower() or '–ª—Å' in value.lower():
                if not car_data["power"]:
                    car_data["power"] = value
                    logger.debug(f"   ‚úÖ –ú–æ—â–Ω–æ—Å—Ç—å: {value}")
        elif classification == "mileage" and not car_data["mileage"]:
            car_data["mileage"] = self._extract_number(value)
        elif classification == "color" and not car_data["color"]:
            car_data["color"] = value
        elif classification == "location" and not car_data["city"]:
            car_data["city"] = value
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            car_data["characteristics"][key] = value
    
    def _extract_from_catalog_card(self, card_element, url: str) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –∞–≤—Ç–æ–º–æ–±–∏–ª–µ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ –ø–∞—Ä—Å–∏—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–∞—Ç–∞–ª–æ–≥–∞, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        """
        car_data = {
            "source_url": url,
            "mark": None,
            "model": None,
            "city": None,
            "price": None,
            "manufacture_year": None,
            "body_type": None,
            "fuel_type": None,
            "gear_box_type": None,
            "driving_gear_type": None,
            "engine_vol": None,
            "power": None,
            "color": None,
            "mileage": None,
            "characteristics": {},
            "pictures": [],
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–º–∞—Ä–∫–∞ –∏ –º–æ–¥–µ–ª—å)
        title_elem = card_element.find('div', class_=re.compile(r'item__title|js-item-title', re.I))
        if not title_elem:
            title_elem = card_element.find('h2') or card_element.find('h3')
        
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if title_text:
                title_parts = [p for p in title_text.split() if len(p) > 1]
                if len(title_parts) >= 2:
                    car_data["mark"] = title_parts[0]
                    car_data["model"] = " ".join(title_parts[1:min(4, len(title_parts))])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        price_elem = card_element.find('div', class_=re.compile(r'item__price-main|price-main', re.I))
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            extracted_price = self._extract_price(price_text)
            if extracted_price:
                car_data["price"] = extracted_price
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ item__tech
        tech_elem = card_element.find('div', class_=re.compile(r'item__tech|tech', re.I))
        if tech_elem:
            tech_items = tech_elem.find_all('div', recursive=False)
            for item in tech_items:
                text = ''
                for child in item.children:
                    if hasattr(child, 'string') and child.string:
                        text += child.string
                    elif hasattr(child, 'get_text'):
                        child_text = child.get_text(strip=True)
                        if child_text and child_text not in ['\\', '|', '/']:
                            text += ' ' + child_text
                
                if not text:
                    text = item.get_text(strip=True)
                
                text = re.sub(r'[\\|/]', '', text).strip()
                
                if not text or len(text) < 2:
                    continue
                
                # –ì–æ–¥: "2009 –≥."
                if re.search(r'\d{4}\s*–≥\.?', text):
                    year = self._extract_number(text)
                    if year and 1900 <= year <= 2100:
                        car_data["manufacture_year"] = year
                # –ü—Ä–æ–±–µ–≥: "145 000 –∫–º"
                elif '–∫–º' in text.lower():
                    mileage = self._extract_number(text)
                    if mileage:
                        car_data["mileage"] = mileage
                # –û–±—ä–µ–º: "0.8 –ª"
                elif re.search(r'\d+\.?\d*\s*–ª\b', text, re.I):
                    vol_match = re.search(r'(\d+\.?\d*)', text)
                    if vol_match:
                        try:
                            vol = float(vol_match.group(1))
                            car_data["engine_vol"] = int(vol * 1000) if vol < 10 else int(vol)
                        except:
                            pass
                # –ú–æ—â–Ω–æ—Å—Ç—å: "52 –ª.—Å."
                elif '–ª.—Å.' in text.lower() or '–ª—Å' in text.lower():
                    power_text = re.sub(r'–ª\.?—Å\.?', '', text, flags=re.I).strip()
                    car_data["power"] = power_text
                # –¢–æ–ø–ª–∏–≤–æ: "–ë–µ–Ω–∑–∏–Ω", "–î–∏–∑–µ–ª—å"
                elif any(word in text.lower() for word in ['–±–µ–Ω–∑–∏–Ω', '–¥–∏–∑–µ–ª—å', '—ç–ª–µ–∫—Ç—Ä–æ', '–≥–∏–±—Ä–∏–¥']):
                    car_data["fuel_type"] = text
                # –ö–ü–ü: "–ú–µ—Ö–∞–Ω–∏–∫–∞", "–ê–≤—Ç–æ–º–∞—Ç"
                elif any(word in text.lower() for word in ['–º–µ—Ö–∞–Ω–∏–∫', '–∞–≤—Ç–æ–º–∞—Ç', '–≤–∞—Ä–∏–∞—Ç–æ—Ä', '—Ä–æ–±–æ—Ç']):
                    car_data["gear_box_type"] = text
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥
        address_elem = card_element.find('div', class_=re.compile(r'item-row__info-address|address', re.I))
        if address_elem:
            address_text = address_elem.get_text(strip=True)
            # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –≤ –∞–¥—Ä–µ—Å–µ
            city_parts = address_text.split()
            for part in city_parts:
                if len(part) > 3 and not part[0].isdigit():
                    if any(word in part.lower() for word in ['–º–æ—Å–∫–≤', '—Ä–æ—Å—Ç–æ–≤', '—Å–ø–±', '–ø–∏—Ç–µ—Ä', '–∫–∞–∑–∞–Ω', '–Ω–∏–∂–Ω', '–Ω–æ–≤–æ—Å–∏–±', '–µ–∫–∞—Ç–µ—Ä–∏–Ω']):
                        car_data["city"] = re.sub(r'[,;]\s*$', '', part).strip()
                        break
            if not car_data["city"] and address_text:
                car_data["city"] = re.sub(r'[,;]\s*$', '', address_text).strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
        img_elem = card_element.find('img', class_=re.compile(r'lozad|item-row__img', re.I))
        if img_elem:
            img_url = img_elem.get('data-src') or img_elem.get('src')
            if img_url:
                if not img_url.startswith('http'):
                    img_url = urljoin(url, img_url)
                car_data["pictures"].append(img_url)
        
        return car_data
    
    def _log_extracted_data(self, car_data: Dict[str, Any], index: int):
        """
        –í—ã–≤–æ–¥–∏—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ —Ñ–∞–π–ª
        logger.info(f"üìã –ê–í–¢–û–ú–û–ë–ò–õ–¨ #{index}: {car_data.get('mark')} {car_data.get('model')} - "
                   f"–¶–µ–Ω–∞: {car_data.get('price')}, –ì–æ—Ä–æ–¥: {car_data.get('city')}, –ì–æ–¥: {car_data.get('manufacture_year')}")
        
        # –í—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å
        print("\n" + "="*80)
        print(f"üìã –ê–í–¢–û–ú–û–ë–ò–õ–¨ #{index}")
        print("="*80)
        print(f"üîó URL: {car_data.get('source_url', 'N/A')}")
        # –¢–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        car_type = car_data.get('car_type', 'unknown')
        car_type_display = '–ù–æ–≤—ã–π' if car_type == 'new' else '–ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–π' if car_type == 'used' else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
        print(f"üöó –¢–∏–ø: {car_type_display}")
        print(f"üöó –ú–∞—Ä–∫–∞: {car_data.get('mark') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üöô –ú–æ–¥–µ–ª—å: {car_data.get('model') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üìç –ì–æ—Ä–æ–¥: {car_data.get('city') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üí∞ –¶–µ–Ω–∞: {car_data.get('price') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üìÖ –ì–æ–¥: {car_data.get('manufacture_year') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üìè –ü—Ä–æ–±–µ–≥: {car_data.get('mileage') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üöó –¢–∏–ø –∫—É–∑–æ–≤–∞: {car_data.get('body_type') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"‚õΩ –¢–æ–ø–ª–∏–≤–æ: {car_data.get('fuel_type') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"‚öôÔ∏è –ö–ü–ü: {car_data.get('gear_box_type') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üîß –ü—Ä–∏–≤–æ–¥: {car_data.get('driving_gear_type') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è –¥–ª—è –≤—ã–≤–æ–¥–∞
        engine_vol = car_data.get('engine_vol')
        if engine_vol:
            if engine_vol >= 1000:
                vol_litres = engine_vol / 1000.0
                # –ï—Å–ª–∏ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ, –≤—ã–≤–æ–¥–∏–º –±–µ–∑ –¥–µ—Å—è—Ç–∏—á–Ω–æ–π —á–∞—Å—Ç–∏
                if vol_litres == int(vol_litres):
                    vol_display = f"{int(vol_litres)} –ª ({engine_vol} –º–ª)"
                else:
                    vol_display = f"{vol_litres:.1f} –ª ({engine_vol} –º–ª)"
            else:
                vol_display = f"{engine_vol} –º–ª"
            print(f"üîã –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è: {vol_display}")
        else:
            print(f"üîã –û–±—ä–µ–º –¥–≤–∏–≥–∞—Ç–µ–ª—è: –ù–ï –ù–ê–ô–î–ï–ù–û")
        print(f"‚ö° –ú–æ—â–Ω–æ—Å—Ç—å: {car_data.get('power') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        print(f"üé® –¶–≤–µ—Ç: {car_data.get('color') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        if car_data.get('characteristics'):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—é –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å
            equipment = car_data['characteristics'].get('equipment', [])
            if equipment:
                equipment_count = car_data['characteristics'].get('equipment_count', len(equipment))
                print(f"üîß –ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è: {equipment_count} –æ–ø—Ü–∏–π")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ–ø—Ü–∏–π
                for i, option in enumerate(equipment[:5], 1):
                    print(f"   {i}. {option[:70]}...")
                if equipment_count > 5:
                    print(f"   ... –∏ –µ—â–µ {equipment_count - 5} –æ–ø—Ü–∏–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥—Ä—É–≥–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–∏—Å–∫–ª—é—á–∞—è equipment)
            other_chars = {k: v for k, v in car_data['characteristics'].items() 
                          if k not in ['equipment', 'equipment_count', 'car_type']}
            if other_chars:
                print(f"üìù –î–æ–ø. —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {len(other_chars)} —à—Ç.")
                for key, value in list(other_chars.items())[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    if isinstance(value, list):
                        print(f"   - {key}: {len(value)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                    else:
                        print(f"   - {key}: {value}")
        
        # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
        pictures = car_data.get('pictures', [])
        pictures_count = len(pictures)
        print(f"üì∏ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {pictures_count}")
        if pictures_count > 0:
            first_pic = pictures[0]
            if isinstance(first_pic, dict):
                pic_url = first_pic.get('image_url', 'N/A')
            else:
                pic_url = str(first_pic)
            print(f"   –ü–µ—Ä–≤–∞—è: {pic_url[:80]}...")
        
        # NLP –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        if car_data.get('nlp_entities'):
            nlp_entities = car_data['nlp_entities']
            print(f"ü§ñ NLP –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
            if nlp_entities.get('dates'):
                print(f"   - –î–∞—Ç—ã: {nlp_entities['dates']}")
            if nlp_entities.get('locations'):
                print(f"   - –õ–æ–∫–∞—Ü–∏–∏: {nlp_entities['locations']}")
            if nlp_entities.get('products'):
                print(f"   - –ü—Ä–æ–¥—É–∫—Ç—ã: {nlp_entities['products']}")
        
        # Ollama –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        if car_data.get('ollama_extracted'):
            print(f"üß† Ollama –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: –î–∞")
        
        print("="*80 + "\n")
    
    def _parse_car_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ò–ò
        –í–ê–ñ–ù–û: –≠—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –û–¢–î–ï–õ–¨–ù–û–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –∞ –Ω–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        """
        try:
            logger.debug(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {url}")
            
            session = self._create_session()
            response = session.get(url, timeout=10.0)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 404 –æ—à–∏–±–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ - —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
            if response.status_code == 404:
                logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (404): {url}")
                self.stats["total_errors"] += 1
                return None
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –∞ –Ω–µ –∫–∞—Ç–∞–ª–æ–≥
            # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç h1 —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            page_title = soup.find('title')
            page_title_text = page_title.get_text(strip=True) if page_title else ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–∞—Ç–∞–ª–æ–≥–∞
            if '–∫–∞—Ç–∞–ª–æ–≥' in page_title_text.lower() or 'catalog' in page_title_text.lower():
                logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {url}")
                return None
            
            logger.debug(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. Title: {page_title_text[:100]}")
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            self._detect_structure_changes(url, soup)
            
            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è
            logger.info(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {url}")
            car_data = self._intelligent_extract_car_data(soup, url)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–≤—Å–µ–≥–¥–∞)
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: –º–∞—Ä–∫–∞={car_data.get('mark') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}, "
                       f"–º–æ–¥–µ–ª—å={car_data.get('model') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}, "
                       f"—Ü–µ–Ω–∞={car_data.get('price') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}, "
                       f"–≥–æ–¥={car_data.get('manufacture_year') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}, "
                       f"–≥–æ—Ä–æ–¥={car_data.get('city') or '–ù–ï –ù–ê–ô–î–ï–ù–û'}")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è, —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å
            if not car_data.get("mark") or not car_data.get("model"):
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è {url}")
                logger.warning(f"   –ú–∞—Ä–∫–∞: {car_data.get('mark')}, –ú–æ–¥–µ–ª—å: {car_data.get('model')}")
                logger.warning(f"   –≠—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –∏–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ–≤–µ—Ä–Ω—ã")
                
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ URL (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤)
                url_parts = url.rstrip('/').split('/')
                if len(url_parts) >= 5:
                    # –§–æ—Ä–º–∞—Ç: /sale/used/mark/model/id
                    potential_mark = url_parts[-3]
                    potential_model = url_parts[-2]
                    if not car_data.get("mark"):
                        car_data["mark"] = potential_mark.replace('-', ' ').title()
                        logger.info(f"   ‚úÖ –ú–∞—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['mark']}")
                        print(f"   ‚úÖ –ú–∞—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['mark']}")
                    if not car_data.get("model"):
                        car_data["model"] = potential_model.replace('-', ' ').title()
                        logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['model']}")
                        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ URL: {car_data['model']}")
            
            # –í–°–ï–ì–î–ê –≤–æ–∑–≤—Ä–∞—â–∞–µ–º car_data, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ, —á—Ç–æ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–µ): –º–∞—Ä–∫–∞={car_data.get('mark')}, –º–æ–¥–µ–ª—å={car_data.get('model')}")
            return car_data
            
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (404): {url}")
            else:
                logger.error(f"HTTP –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            self.stats["total_errors"] += 1
            return None
        except httpx.TimeoutException:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
            self.stats["total_errors"] += 1
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            self.stats["total_errors"] += 1
            return None
    
    def _find_car_links(self, page_url: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–∞–ª–æ–≥–∞"""
        try:
            session = self._create_session()
            response = session.get(page_url, timeout=10.0)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 404 –æ—à–∏–±–∫–∏
            if response.status_code == 404:
                logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (404): {page_url}")
                return []
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            car_links = []
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è aaa-motors.ru
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º js-item –∏–ª–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ /sale/used/
            link_selectors = [
                # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è aaa-motors.ru (–∫–ª–∞—Å—Å js-item)
                soup.find_all('a', class_=re.compile(r'js-item|item-row', re.I)),
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ href
                soup.find_all('a', href=re.compile(r'/sale/(used|new)/', re.I)),
                soup.find_all('a', href=re.compile(r'/car/|/auto/|/vehicle/|/offer/', re.I)),
                soup.find_all('a', class_=re.compile(r'car|auto|vehicle|offer|card', re.I)),
            ]
            
            for selector_list in link_selectors:
                for link in selector_list:
                    href = link.get('href')
                    if not href:
                        continue
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = urljoin(self.base_url, href)
                    elif not href.startswith('http'):
                        href = urljoin(self.base_url, href)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—å
                    # –§–æ—Ä–º–∞—Ç aaa-motors.ru: /sale/used/daewoo/matiz/cd64d5
                    if any(pattern in href.lower() for pattern in [
                        '/sale/used/', '/sale/new/', '/car/', '/auto/', '/vehicle/', '/offer/'
                    ]):
                        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
                        if '?' in href:
                            href = href.split('?')[0]
                        if '#' in href:
                            href = href.split('#')[0]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å (–º–∏–Ω–∏–º—É–º 2 —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ /sale/used/)
                        parts = href.rstrip('/').split('/')
                        if len(parts) >= 5:  # https://, domain, sale, used, mark, model, id
                            car_links.append(href)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_links = []
            for link in car_links:
                if link not in seen:
                    seen.add(link)
                    unique_links.append(link)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url}")
            return unique_links
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url}: {e}")
            return []
    
    def _find_catalog_pages(self) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        catalog_pages = []
        
        try:
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ URL –¥–ª—è aaa-motors.ru
            catalog_urls = [
                f"{self.base_url}/catalog",  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥
                f"{self.base_url}/sale/used",  # –ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
                f"{self.base_url}/sale/new",  # –ù–æ–≤—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
                f"{self.base_url}/cars",
                f"{self.base_url}/auto",
                f"{self.base_url}/offers",
                f"{self.base_url}/",
            ]
            
            session = self._create_session()
            for catalog_url in catalog_urls:
                try:
                    response = session.get(catalog_url)
                    if response.status_code == 200:
                        catalog_pages.append(catalog_url)
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–∞—Ç–∞–ª–æ–≥: {catalog_url}")
                        
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # –ò—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
                        pagination = soup.find_all('a', href=re.compile(r'page|p=\d+|/catalog\?page', re.I))
                        for page_link in pagination:
                            href = page_link.get('href')
                            if href:
                                if href.startswith('//'):
                                    href = 'https:' + href
                                elif href.startswith('/'):
                                    href = urljoin(self.base_url, href)
                                elif not href.startswith('http'):
                                    href = urljoin(self.base_url, href)
                                if href not in catalog_pages:
                                    catalog_pages.append(href)
                        
                        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                        if '/catalog' in catalog_url:
                            break
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {catalog_url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞: {e}")
        
        if not catalog_pages:
            # Fallback –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥
            catalog_pages = [f"{self.base_url}/catalog"]
            logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback URL –∫–∞—Ç–∞–ª–æ–≥–∞")
        
        return catalog_pages
    
    def _save_car(self, car_data: Dict[str, Any]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
            logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {car_data.get('mark')} {car_data.get('model')} - "
                       f"URL: {car_data.get('source_url')}")
            
            existing = self.db.query(ParsedCar).filter(
                ParsedCar.source_url == car_data["source_url"]
            ).first()
            
            if existing:
                logger.debug(f"   üîÑ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∑–∞–ø–∏—Å—å (ID: {existing.id}), –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ...")
                logger.info(f"   üìä –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ –ë–î: mark={existing.mark}, model={existing.model}, price={existing.price}")
                logger.info(f"   üìä –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: mark={car_data.get('mark')}, model={car_data.get('model')}, price={car_data.get('price')}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –î–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –í–°–ï–ì–î–ê –æ–±–Ω–æ–≤–ª—è–µ–º, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—É—Å—Ç–æ–µ
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–º–∏
                for key, value in car_data.items():
                    if key not in ["pictures", "characteristics", "sentiment", "nlp_entities", "ollama_extracted", "source_url", "car_type"] and hasattr(existing, key):
                        existing_value = getattr(existing, key, None)
                        should_update = False
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ–µ
                        if value is not None and value != '':
                            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ 0 (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—Ä–æ–±–µ–≥ –∏–ª–∏ –≥–æ–¥)
                            if isinstance(value, (int, float)):
                                if key in ['mileage', 'manufacture_year'] or value > 0:
                                    should_update = True
                            else:
                                should_update = True
                        
                        # –í–°–ï–ì–î–ê –æ–±–Ω–æ–≤–ª—è–µ–º –µ—Å–ª–∏ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º)
                        if should_update:
                            # –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π (mark, model, price, year) –í–°–ï–ì–î–ê –æ–±–Ω–æ–≤–ª—è–µ–º, –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –ø—É—Å—Ç–æ–µ
                            if key in ['mark', 'model', 'price', 'manufacture_year']:
                                # –ï—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—É—Å—Ç–æ–µ –ò–õ–ò –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –Ω–æ–≤–æ–≥–æ - –æ–±–Ω–æ–≤–ª—è–µ–º
                                if (existing_value is None or existing_value == '' or existing_value == 0) or (existing_value != value):
                                    logger.info(f"      ‚úèÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ –ø–æ–ª—è {key}: '{existing_value}' -> '{value}'")
                                    setattr(existing, key, value)
                            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –ø—É—Å—Ç–æ–µ
                            elif existing_value is None or existing_value == '' or existing_value == 0:
                                logger.debug(f"      ‚úèÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è {key}: '{existing_value}' -> '{value}'")
                                setattr(existing, key, value)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–≤–∫–ª—é—á–∞—è —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—é)
                if car_data.get("characteristics"):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è –≤ characteristics, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    if car_data.get("car_type"):
                        car_data["characteristics"]["car_type"] = car_data["car_type"]
                    existing.characteristics = json.dumps(car_data["characteristics"], ensure_ascii=False)
                elif car_data.get("car_type"):
                    # –ï—Å–ª–∏ characteristics –ø—É—Å—Ç—ã–µ, –Ω–æ –µ—Å—Ç—å —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è
                    existing.characteristics = json.dumps({"car_type": car_data["car_type"]}, ensure_ascii=False)
                
                self.db.query(ParsedCarPicture).filter(
                    ParsedCarPicture.parsed_car_id == existing.id
                ).delete()
                
                parsed_car = existing
                logger.debug(f"   ‚úÖ –ó–∞–ø–∏—Å—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞ (ID: {parsed_car.id})")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                logger.debug(f"   ‚ûï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏...")
                parsed_car = ParsedCar(
                    source_url=car_data["source_url"],
                    mark=car_data.get("mark"),
                    model=car_data.get("model"),
                    city=car_data.get("city"),
                    price=car_data.get("price"),
                    manufacture_year=car_data.get("manufacture_year"),
                    body_type=car_data.get("body_type"),
                    fuel_type=car_data.get("fuel_type"),
                    gear_box_type=car_data.get("gear_box_type"),
                    driving_gear_type=car_data.get("driving_gear_type"),
                    engine_vol=car_data.get("engine_vol"),
                    power=car_data.get("power"),
                    color=car_data.get("color"),
                    mileage=car_data.get("mileage"),
                    characteristics=None,  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è car_type
                    is_active=True
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –∞–≤—Ç–æ–º–æ–±–∏–ª—è –≤ characteristics –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                char_dict = car_data.get("characteristics", {})
                if car_data.get("car_type"):
                    char_dict["car_type"] = car_data["car_type"]
                
                if char_dict:
                    parsed_car.characteristics = json.dumps(char_dict, ensure_ascii=False)
                
                # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                if not parsed_car.mark and car_data.get("mark"):
                    parsed_car.mark = car_data["mark"]
                    logger.warning(f"   üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ mark: {car_data['mark']}")
                if not parsed_car.model and car_data.get("model"):
                    parsed_car.model = car_data["model"]
                    logger.warning(f"   üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ model: {car_data['model']}")
                if not parsed_car.price and car_data.get("price"):
                    parsed_car.price = car_data["price"]
                    logger.warning(f"   üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ price: {car_data['price']}")
                if not parsed_car.manufacture_year and car_data.get("manufacture_year"):
                    parsed_car.manufacture_year = car_data["manufacture_year"]
                    logger.warning(f"   üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ year: {car_data['manufacture_year']}")
                if not parsed_car.body_type and car_data.get("body_type"):
                    parsed_car.body_type = car_data["body_type"]
                if not parsed_car.fuel_type and car_data.get("fuel_type"):
                    parsed_car.fuel_type = car_data["fuel_type"]
                if not parsed_car.gear_box_type and car_data.get("gear_box_type"):
                    parsed_car.gear_box_type = car_data["gear_box_type"]
                if not parsed_car.city and car_data.get("city"):
                    parsed_car.city = car_data["city"]
                
                self.db.add(parsed_car)
                self.db.flush()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
                logger.info(f"   ‚úÖ –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏ ID={parsed_car.id}: mark={parsed_car.mark}, model={parsed_car.model}, price={parsed_car.price}, year={parsed_car.manufacture_year}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            for pic_data in car_data.get("pictures", []):
                picture = ParsedCarPicture(
                    parsed_car_id=parsed_car.id,
                    image_url=pic_data["image_url"],
                    seqno=pic_data.get("seqno", 0)
                )
                self.db.add(picture)
            
            self.db.commit()
            self.stats["total_parsed"] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –∏–∑ –ë–î –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            self.db.refresh(parsed_car)
            
            logger.info(f"   ‚úÖ –ê–≤—Ç–æ–º–æ–±–∏–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î (ID: {parsed_car.id})")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å
            logger.debug(f"   üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: "
                       f"–º–∞—Ä–∫–∞={parsed_car.mark}, –º–æ–¥–µ–ª—å={parsed_car.model}, "
                       f"—Ü–µ–Ω–∞={parsed_car.price}, –≥–æ–¥={parsed_car.manufacture_year}")
            
            # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            critical_fields = ['mark', 'model', 'price', 'manufacture_year']
            missing_fields = []
            for field in critical_fields:
                current_value = getattr(parsed_car, field, None)
                data_value = car_data.get(field)
                # –ï—Å–ª–∏ –≤ –ë–î –ø—É—Å—Ç–æ, –∞ –≤ car_data –µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                if not current_value and data_value:
                    setattr(parsed_car, field, data_value)
                    missing_fields.append(field)
                    logger.warning(f"   üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—è {field}: {data_value}")
                # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –µ—Å—Ç—å –≤ car_data, –Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è - –æ–±–Ω–æ–≤–ª—è–µ–º
                elif current_value != data_value and data_value:
                    setattr(parsed_car, field, data_value)
                    missing_fields.append(field)
                    logger.warning(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è {field}: '{current_value}' -> '{data_value}'")
            
            if missing_fields:
                self.db.commit()
                self.db.refresh(parsed_car)
                logger.info(f"   ‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã/–æ–±–Ω–æ–≤–ª–µ–Ω—ã –ø–æ–ª—è: {missing_fields}")
            
            return True
            
        except IntegrityError as e:
            self.db.rollback()
            logger.warning(f"–î—É–±–ª–∏–∫–∞—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è {car_data.get('source_url')}: {e}")
            return False
        except Exception as e:
            self.db.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {e}")
            self.stats["total_errors"] += 1
            return False
    
    def clear_all_data(self) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü –ø–∞—Ä—Å–∏–Ω–≥–∞ (–≤–∫–ª—é—á–∞—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ (–í–°–ï, –≤–∫–ª—é—á–∞—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ)
            cars_count = self.db.query(ParsedCar).count()
            pictures_count = self.db.query(ParsedCarPicture).count()
            
            logger.info(f"üóëÔ∏è –ù–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {cars_count} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {pictures_count} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
            print(f"\n{'='*80}")
            print(f"üóëÔ∏è –ù–ê–ß–ê–õ–û –û–ß–ò–°–¢–ö–ò –î–ê–ù–ù–´–•")
            print(f"{'='*80}")
            print(f"–ù–∞–π–¥–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {cars_count}")
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {pictures_count}")
            
            if cars_count == 0 and pictures_count == 0:
                logger.info("‚úÖ –î–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—Ç, –±–∞–∑–∞ —É–∂–µ –ø—É—Å—Ç–∞")
                print(f"‚úÖ –î–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—Ç")
                print(f"{'='*80}\n")
                return 0
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–¥–æ–ª–∂–Ω—ã —É–¥–∞–ª—è—Ç—å—Å—è –ø–µ—Ä–≤—ã–º–∏ –∏–∑-–∑–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º synchronize_session=False –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∫–ª—é—á–∞–º–∏
            deleted_pictures = self.db.query(ParsedCarPicture).delete(synchronize_session=False)
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {deleted_pictures}")
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {deleted_pictures}")
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ (–í–°–ï, –≤–∫–ª—é—á–∞—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ)
            deleted_cars = self.db.query(ParsedCar).delete(synchronize_session=False)
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {deleted_cars}")
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π: {deleted_cars}")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
            self.db.commit()
            logger.info(f"‚úÖ –ö–æ–º–º–∏—Ç –æ—á–∏—Å—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–µ–Ω—ã
            remaining_cars = self.db.query(ParsedCar).count()
            remaining_pictures = self.db.query(ParsedCarPicture).count()
            
            if remaining_cars > 0 or remaining_pictures > 0:
                logger.warning(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–∏—Ç—å –µ—â–µ —Ä–∞–∑
                try:
                    logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π...")
                    self.db.query(ParsedCarPicture).delete(synchronize_session=False)
                    self.db.query(ParsedCar).delete(synchronize_session=False)
                    self.db.commit()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑
                    remaining_cars = self.db.query(ParsedCar).count()
                    remaining_pictures = self.db.query(ParsedCarPicture).count()
                    if remaining_cars == 0 and remaining_pictures == 0:
                        logger.info(f"‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                        print(f"‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                    else:
                        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞–ª–æ—Å—å: {remaining_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {remaining_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è: {e}", exc_info=True)
            else:
                logger.info(f"‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã")
                print(f"‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã")
            
            logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {deleted_cars} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, {deleted_pictures} —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
            print(f"{'='*80}")
            print(f"‚úÖ –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê")
            print(f"{'='*80}\n")
            return deleted_cars
        except Exception as e:
            self.db.rollback()
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –û–ß–ò–°–¢–ö–ï –î–ê–ù–ù–´–•: {e}\n")
            raise
    
    def parse(self, max_pages: Optional[int] = None, max_cars: Optional[int] = None, delay: float = 1.0, clear_before: bool = True) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        
        Args:
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞
            max_cars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
            clear_before: –û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        """
        self.is_running = True
        self.stats = {
            "total_parsed": 0,
            "total_errors": 0,
            "current_page": 0,
            "nlp_extractions": 0,
            "structure_changes_detected": 0,
            "ollama_extractions": 0  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è Ollama
        }
        
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é clear_before=True, –µ—Å–ª–∏ —è–≤–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ False
            should_clear = clear_before if clear_before is not None else True
            
            logger.info(f"üóëÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä clear_before: {clear_before} (—Ä–µ–∑—É–ª—å—Ç–∞—Ç: should_clear={should_clear})")
            
            if should_clear:
                print("\n" + "="*80)
                print("üóëÔ∏è –ù–ê–ß–ê–õ–û –û–ß–ò–°–¢–ö–ò –î–ê–ù–ù–´–• –ü–ï–†–ï–î –ü–ê–†–°–ò–ù–ì–û–ú")
                print("="*80)
                logger.info("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º...")
                logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä clear_before={clear_before}, should_clear={should_clear} - –æ—á–∏—Å—Ç–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                deleted_count = self.clear_all_data()
                
                logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø–∞—Ä—Å–∏–Ω–≥–∞")
                print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£–¥–∞–ª–µ–Ω–æ {deleted_count} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π. –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥...\n")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–¥–∞–ª–µ–Ω—ã
                remaining_after_clear = self.db.query(ParsedCar).count()
                if remaining_after_clear > 0:
                    logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {remaining_after_clear} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π!")
                    print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {remaining_after_clear} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π!")
                    # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è
                    try:
                        self.db.query(ParsedCar).delete(synchronize_session=False)
                        self.db.query(ParsedCarPicture).delete(synchronize_session=False)
                        self.db.commit()
                        logger.info(f"‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–µ–Ω—ã –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø–∏—Å–∏")
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è: {e}")
                else:
                    logger.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞, –≥–æ—Ç–æ–≤ –∫ –ø–∞—Ä—Å–∏–Ω–≥—É")
            else:
                logger.warning("‚ö†Ô∏è –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∫–ª—é—á–µ–Ω–∞ (clear_before=False). –î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º.")
                print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∫–ª—é—á–µ–Ω–∞. –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º.\n")
            
            catalog_pages = self._find_catalog_pages()
            if not catalog_pages:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞")
                return {
                    "status": "error",
                    "message": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞",
                    **self.stats
                }
            
            if max_pages:
                catalog_pages = catalog_pages[:max_pages]
            
            all_car_links = []
            
            for page_url in catalog_pages:
                if not self.is_running:
                    break
                    
                self.stats["current_page"] += 1
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {self.stats['current_page']}: {page_url}")
                
                car_links = self._find_car_links(page_url)
                all_car_links.extend(car_links)
                
                if delay > 0:
                    time.sleep(delay)
            
            unique_car_links = list(set(all_car_links))
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_car_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
            
            if max_cars:
                unique_car_links = unique_car_links[:max_cars]
            
            for idx, car_url in enumerate(unique_car_links):
                if not self.is_running:
                    break
                
                if max_cars and self.stats["total_parsed"] >= max_cars:
                    break
                
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∞–≤—Ç–æ–º–æ–±–∏–ª—è {idx + 1}/{len(unique_car_links)}: {car_url}")
                
                car_data = self._parse_car_page(car_url)
                
                # –í–°–ï–ì–î–ê –≤—ã–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ–ø–æ–ª–Ω—ã–µ
                if car_data:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö–æ—Ç—è –±—ã –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã
                    has_basic_data = any([
                        car_data.get('mark'),
                        car_data.get('model'),
                        car_data.get('price'),
                        car_data.get('manufacture_year')
                    ])
                    
                    if not has_basic_data:
                        logger.warning(f"‚ö†Ô∏è [{idx + 1}/{len(unique_car_links)}] –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –Ω–æ –≤—Å–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ: {car_url}")
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ö–æ—Ç—è –±—ã –º–∞—Ä–∫—É –∏ –º–æ–¥–µ–ª—å –∏–∑ URL
                        url_parts = car_url.rstrip('/').split('/')
                        if len(url_parts) >= 5:
                            if not car_data.get('mark'):
                                car_data['mark'] = url_parts[-3].replace('-', ' ').title()
                            if not car_data.get('model'):
                                car_data['model'] = url_parts[-2].replace('-', ' ').title()
                            logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –º–∞—Ä–∫–∞ –∏ –º–æ–¥–µ–ª—å –∏–∑ URL: {car_data.get('mark')} {car_data.get('model')}")
                    
                    # –í—ã–≤–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–í–°–ï–ì–î–ê)
                    logger.info(f"üìã [{idx + 1}/{len(unique_car_links)}] –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {car_url}")
                    self._log_extracted_data(car_data, idx + 1)
                    
                    # –í–°–ï–ì–î–ê –ø—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–º–Ω–æ–≥–æ
                    logger.info(f"üì¶ –ü–ï–†–ï–î–ê–ß–ê –î–ê–ù–ù–´–• –í _save_car:")
                    logger.info(f"   mark={car_data.get('mark')}, model={car_data.get('model')}, price={car_data.get('price')}")
                    logger.info(f"   year={car_data.get('manufacture_year')}, city={car_data.get('city')}")
                    logger.info(f"   body_type={car_data.get('body_type')}, fuel_type={car_data.get('fuel_type')}, gear_box={car_data.get('gear_box_type')}")
                    
                    saved = self._save_car(car_data)
                    if saved:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å
                        saved_car = self.db.query(ParsedCar).filter(
                            ParsedCar.source_url == car_data['source_url']
                        ).first()
                        
                        if saved_car:
                            logger.info(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: mark={saved_car.mark}, model={saved_car.model}, price={saved_car.price}")
                            
                            # –ö–†–ò–¢–ò–ß–ù–û: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º
                            needs_update = False
                            if not saved_car.mark and car_data.get("mark"):
                                saved_car.mark = car_data["mark"]
                                needs_update = True
                                logger.warning(f"   üîÑ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ mark: {car_data['mark']}")
                            if not saved_car.model and car_data.get("model"):
                                saved_car.model = car_data["model"]
                                needs_update = True
                                logger.warning(f"   üîÑ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ model: {car_data['model']}")
                            if not saved_car.price and car_data.get("price"):
                                saved_car.price = car_data["price"]
                                needs_update = True
                                logger.warning(f"   üîÑ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ price: {car_data['price']}")
                            if not saved_car.manufacture_year and car_data.get("manufacture_year"):
                                saved_car.manufacture_year = car_data["manufacture_year"]
                                needs_update = True
                                logger.warning(f"   üîÑ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ year: {car_data['manufacture_year']}")
                            if not saved_car.body_type and car_data.get("body_type"):
                                saved_car.body_type = car_data["body_type"]
                                needs_update = True
                            if not saved_car.fuel_type and car_data.get("fuel_type"):
                                saved_car.fuel_type = car_data["fuel_type"]
                                needs_update = True
                            if not saved_car.gear_box_type and car_data.get("gear_box_type"):
                                saved_car.gear_box_type = car_data["gear_box_type"]
                                needs_update = True
                            
                            if needs_update:
                                self.db.commit()
                                self.db.refresh(saved_car)
                                logger.info(f"   ‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: mark={saved_car.mark}, model={saved_car.model}, price={saved_car.price}")
                        else:
                            logger.warning(f"   ‚ö†Ô∏è –ó–∞–ø–∏—Å—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {car_url}")
                else:
                    logger.warning(f"‚ö†Ô∏è [{idx + 1}/{len(unique_car_links)}] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ: {car_url}")
                    print(f"‚ö†Ô∏è [{idx + 1}/{len(unique_car_links)}] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ: {car_url}")
                    print(f"   –≠—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å\n")
                    self.stats["total_errors"] += 1
                
                if delay > 0 and idx < len(unique_car_links) - 1:
                    time.sleep(delay)
            
            message = f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.stats['total_parsed']} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π. "
            message += f"NLP –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {self.stats['nlp_extractions']}. "
            if self.use_ollama:
                message += f"Ollama –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {self.stats['ollama_extractions']}. "
            message += f"–ò–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {self.stats['structure_changes_detected']}"
            
            return {
                "status": "completed",
                "message": message,
                **self.stats
            }
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return {
                "status": "error",
                "message": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}",
                **self.stats
            }
        finally:
            self.is_running = False
            if self.session:
                self.session.close()
                self.session = None
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥"""
        self.is_running = False
    
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        return {
            "status": "running" if self.is_running else "stopped",
            **self.stats
        }

